{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network (now predicts only the median)\n",
    "class NextStateMedianNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(NextStateMedianNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, state_dim)  # Single output per state dim\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        if len(state.shape) == 1:\n",
    "            x = torch.cat((action, state))\n",
    "        else:\n",
    "            x = torch.cat((action, state), dim=1) # .unsqueeze(1)\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "def quantile_loss_median(predicted, target):\n",
    "    error = target - predicted\n",
    "    quantile = torch.tensor(0.5)\n",
    "    loss = torch.max(\n",
    "        quantile * error,\n",
    "        (quantile - 1) * error\n",
    "    )\n",
    "    return loss.mean()\n",
    "\n",
    "def mse_loss(predicted, target):\n",
    "    error = target - predicted\n",
    "    return error.pow(2).mean()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test where I removed the quadratic part, need to put the threshold back to 1 to back to standard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NextStateQuantileNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, num_quantiles):\n",
    "        super(NextStateQuantileNetwork, self).__init__()\n",
    "        self.num_quantiles = num_quantiles\n",
    "\n",
    "        # Input layer (state + action concatenation)\n",
    "        self.layer1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.layer2 = nn.Linear(256, 256)\n",
    "        self.layer3 = nn.Linear(256, state_dim * num_quantiles)  # Output quantiles for each state dimension\n",
    "        # self.layer3 = torch.tanh(256, state_dim * num_quantiles)  # Output quantiles for each state dimension\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        # Concatenate state and action\n",
    "        # x = torch.cat((action, state))\n",
    "        # print(\"action \", action, \"\\n\")\n",
    "        \n",
    "        # print(\"state \", state, \"\\n\")\n",
    "        # print(\"state.shape \", state.shape, \"\\n\")\n",
    "        # print(\"action \", action, \"\\n\")\n",
    "        # print(\"action.shape \", action.shape, \"\\n\")\n",
    "        \n",
    "        if len(state.shape) == 1:\n",
    "            x = torch.cat((action, state))\n",
    "        else:\n",
    "            x = torch.cat((action, state), dim=1) # .unsqueeze(1)\n",
    "            \n",
    "        # print(\"x \", x, \"\\n\")\n",
    "        # print(\"x.shape \", x.shape, \"\\n\")\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        x = torch.relu(self.layer2(x))\n",
    "        x = self.layer3(x)\n",
    "        return x.view(-1, self.num_quantiles, state.size(-1))\n",
    "\n",
    "\n",
    "def quantile_huber_loss(predicted, target, quantiles, batch_size=32):\n",
    "    \"\"\"\n",
    "    Calculate Quantile Huber Loss.\n",
    "    :param predicted: Predicted quantiles, shape (batch_size, state_dim, num_quantiles)\n",
    "    :param target: Target next state, shape (batch_size, state_dim)\n",
    "    :param quantiles: Quantiles (e.g., [0.1, 0.3, 0.7, 0.9]), shape (num_quantiles,)\n",
    "    \"\"\"\n",
    "    \n",
    "    # print(\"target.shape \", target.shape, \"\\n\")\n",
    "    # print(\"target \", target, \"\\n\")\n",
    "    # target = target.unsqueeze(-1)  # Shape: (batch_size, state_dim, 1)\n",
    "    target = target.unsqueeze(1).repeat(1,len(quantiles),1)\n",
    "    # print(\"target \", target, \"\\n\")\n",
    "    # print(\"predicted \", predicted, \"\\n\")\n",
    "    # print(\"target.shape \", target.shape, \"\\n\")\n",
    "    # print(\"predicted.shape \", predicted.shape, \"\\n\")\n",
    "    \n",
    "    error = target - predicted  # Shape: (batch_size, state_dim, num_quantiles)\n",
    "    \n",
    "    # print(\"error \", error, \"\\n\")\n",
    "    quantiles = quantiles.view(1, -1, 1)\n",
    "    quantiles = quantiles.repeat(batch_size, 1, target.shape[-1])  # Shape: [3, 4, 2]\n",
    "    # quantiles = quantiles.repeat(batch_size, target.shape[-1], 1) \n",
    "    # quantiles = quantiles.transpose(1, 2)  # Shape: [3, 2, 4]\n",
    "    # print(\"quantiles \", quantiles, \"\\n\")\n",
    "    \n",
    "    # # Make delta adaptive by scaling it based on quantiles\n",
    "    # delta = 1.0\n",
    "    # adaptive_delta = delta * (1.0 + torch.abs(quantiles - 0.5))  # Give more tolerance to extreme quantiles\n",
    "    \n",
    "    # # print(\"quantiles.shape \", quantiles.shape, \"\\n\")\n",
    "    # # Calculate loss\n",
    "    # huber_loss = torch.where(\n",
    "    #     error.abs() <= 0.0, # 1.0\n",
    "    #     0.5 * error.pow(2),\n",
    "    #     error.abs() - 0.5\n",
    "    # )\n",
    "\n",
    "    # # huber_loss = error.abs()-0.5  # Simple L1 loss\n",
    "    \n",
    "    # # print(\"huber_loss \", huber_loss, \"\\n\")\n",
    "    # # print(\"huber_loss.shape \", huber_loss.shape, \"\\n\")\n",
    "    \n",
    "    # # Quantile loss computation\n",
    "    # quantile_loss = (quantiles - (error < 0).float()).abs() * huber_loss\n",
    "\n",
    "    # Standard Quantile Loss (Pinball Loss)\n",
    "    quantile_loss = torch.max(\n",
    "        quantiles * error,\n",
    "        (quantiles - 1) * error\n",
    "    )\n",
    "\n",
    "    return quantile_loss.mean()\n",
    "\n",
    "    # Quantile loss computation\n",
    "    # quantile_loss = torch.abs(quantiles - (error.detach() < 0).float()) * huber_loss\n",
    "    # return quantile_loss.sum(dim=1).mean()\n",
    "\n",
    "# def quantile_huber_loss(predicted, target, quantiles, batch_size=32):\n",
    "#     \"\"\"\n",
    "#     Calculate Quantile Huber Loss.\n",
    "#     :param predicted: Predicted quantiles, shape (batch_size, state_dim, num_quantiles)\n",
    "#     :param target: Target next state, shape (batch_size, state_dim)\n",
    "#     :param quantiles: Quantiles (e.g., [0.1, 0.3, 0.7, 0.9]), shape (num_quantiles,)\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # print(\"target.shape \", target.shape, \"\\n\")\n",
    "#     # print(\"target \", target, \"\\n\")\n",
    "#     # target = target.unsqueeze(-1)  # Shape: (batch_size, state_dim, 1)\n",
    "#     target = target.unsqueeze(1).repeat(1,len(quantiles),1)\n",
    "#     print(\"target \", target, \"\\n\")\n",
    "#     print(\"predicted \", predicted, \"\\n\")\n",
    "#     print(\"target.shape \", target.shape, \"\\n\")\n",
    "#     print(\"predicted.shape \", predicted.shape, \"\\n\")\n",
    "    \n",
    "#     error = target - predicted  # Shape: (batch_size, state_dim, num_quantiles)\n",
    "    \n",
    "#     # print(\"error \", error, \"\\n\")\n",
    "#     quantiles = quantiles.view(1, -1, 1)\n",
    "#     quantiles = quantiles.repeat(batch_size, 1, target.shape[-1])  # Shape: [3, 4, 2]\n",
    "#     # quantiles = quantiles.repeat(batch_size, target.shape[-1], 1) \n",
    "#     # quantiles = quantiles.transpose(1, 2)  # Shape: [3, 2, 4]\n",
    "#     print(\"quantiles \", quantiles, \"\\n\")\n",
    "    \n",
    "#     # print(\"quantiles.shape \", quantiles.shape, \"\\n\")\n",
    "#     # Calculate loss\n",
    "#     huber_loss = torch.where(\n",
    "#         error.abs() <= 1.0,\n",
    "#         0.5 * error.pow(2),\n",
    "#         error.abs() - 0.5\n",
    "#     )\n",
    "    \n",
    "#     # print(\"huber_loss \", huber_loss, \"\\n\")\n",
    "#     # print(\"huber_loss.shape \", huber_loss.shape, \"\\n\")\n",
    "    \n",
    "#     # Quantile loss computation\n",
    "#     # quantile_loss = (quantiles - (error < 0).float()).abs() * huber_loss\n",
    "#     # return quantile_loss.mean()\n",
    "\n",
    "#     # Quantile loss computation\n",
    "#     quantile_loss = torch.abs(quantiles - (error.detach() < 0).float()) * huber_loss\n",
    "#     return quantile_loss.sum(dim=1).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph of number of values below the different quantiles graph and calculation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_nb_below_quantiles(nb_belowq1_list, nb_belowq2_list, nb_belowq3_list, nb_belowq4_list, nb_belowq5_list, nb_belowq6_list, nb_belowq7_list, nb_belowq8_list, nb_belowq9_list, nb_belowq10_list, variable):\n",
    "    \n",
    "    if variable == \"theta\":\n",
    "        variable = r\"$\\theta$\"\n",
    "\n",
    "    plt.figure(1)\n",
    "    # plt.plot(nb_belowq1_list, label=f'quantile1_{variable}')\n",
    "    # plt.plot(nb_belowq2_list, label=f'quantile2_{variable}')\n",
    "    # plt.plot(nb_belowq3_list, label=f'quantile3_{variable}')\n",
    "    # plt.plot(nb_belowq4_list, label=f'quantile4_{variable}')\n",
    "    # plt.plot(nb_belowq5_list, label=f'quantile5_{variable}')\n",
    "    # plt.plot(nb_belowq6_list, label=f'quantile6_{variable}')\n",
    "    # plt.plot(nb_belowq7_list, label=f'quantile7_{variable}')\n",
    "    # plt.plot(nb_belowq8_list, label=f'quantile8_{variable}')\n",
    "    # plt.plot(nb_belowq9_list, label=f'quantile9_{variable}')\n",
    "    # plt.plot(nb_belowq10_list, label=f'quantile10_{variable}')\n",
    "    plt.plot(nb_belowq1_list, label=f'Q1_{variable}')\n",
    "    plt.plot(nb_belowq2_list, label=f'Q2_{variable}')\n",
    "    plt.plot(nb_belowq3_list, label=f'Q3_{variable}')\n",
    "    plt.plot(nb_belowq4_list, label=f'Q4_{variable}')\n",
    "    plt.plot(nb_belowq5_list, label=f'Q5_{variable}')\n",
    "    plt.plot(nb_belowq6_list, label=f'Q6_{variable}')\n",
    "    plt.plot(nb_belowq7_list, label=f'Q7_{variable}')\n",
    "    plt.plot(nb_belowq8_list, label=f'Q8_{variable}')\n",
    "    plt.plot(nb_belowq9_list, label=f'Q9_{variable}')\n",
    "    plt.plot(nb_belowq10_list, label=f'Q10_{variable}')\n",
    "    plt.hlines(0.1, 0, len(nb_belowq1_list), colors='r', linestyles='dashed') # , label='quantile1'\n",
    "    plt.hlines(0.2, 0, len(nb_belowq1_list), colors='r', linestyles='dashed') # , label='quantile2'\n",
    "    plt.hlines(0.3, 0, len(nb_belowq1_list), colors='r', linestyles='dashed') # , label='quantile3'\n",
    "    plt.hlines(0.4, 0, len(nb_belowq1_list), colors='r', linestyles='dashed') # , label='quantile4'\n",
    "    plt.hlines(0.5, 0, len(nb_belowq1_list), colors='r', linestyles='dashed') # , label='quantile5'\n",
    "    plt.hlines(0.6, 0, len(nb_belowq1_list), colors='r', linestyles='dashed') # , label='quantile6'\n",
    "    plt.hlines(0.7, 0, len(nb_belowq1_list), colors='r', linestyles='dashed') # , label='quantile7'\n",
    "    plt.hlines(0.8, 0, len(nb_belowq1_list), colors='r', linestyles='dashed') # , label='quantile8'\n",
    "    plt.hlines(0.9, 0, len(nb_belowq1_list), colors='r', linestyles='dashed') # , label='quantile9'\n",
    "    plt.hlines(1.0, 0, len(nb_belowq1_list), colors='r', linestyles='dashed') # , label='quantile10'\n",
    "    plt.xlabel('Number of steps')\n",
    "    plt.ylabel(f'Number of steps below quantile {variable}')\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_below_quantile(env, quantile0, quantile1, quantile2, quantile3, quantile4, quantile5, quantile6, quantile7, quantile8, quantile9, quantile10):\n",
    "    \n",
    "    nb_belowq1 = 0\n",
    "    nb_belowq2 = 0\n",
    "    nb_belowq3 = 0\n",
    "    nb_belowq4 = 0\n",
    "    nb_belowq5 = 0\n",
    "    nb_belowq6 = 0\n",
    "    nb_belowq7 = 0\n",
    "    nb_belowq8 = 0\n",
    "    nb_belowq9 = 0\n",
    "    nb_belowq10 = 0\n",
    "    \n",
    "    nb_belowq1_list = [0]\n",
    "    nb_belowq2_list = [0]\n",
    "    nb_belowq3_list = [0]\n",
    "    nb_belowq4_list = [0]\n",
    "    nb_belowq5_list = [0]\n",
    "    nb_belowq6_list = [0]\n",
    "    nb_belowq7_list = [0]\n",
    "    nb_belowq8_list = [0]\n",
    "    nb_belowq9_list = [0]\n",
    "    nb_belowq10_list = [0]\n",
    "    \n",
    "    \n",
    "    for i in range(len(quantile0)):\n",
    "        \n",
    "        if env[i] < quantile1[i]:\n",
    "            nb_belowq1 += 1\n",
    "            \n",
    "        if env[i] < quantile2[i]:\n",
    "            nb_belowq2 += 1\n",
    "            \n",
    "        if env[i] < quantile3[i]:\n",
    "            nb_belowq3 += 1\n",
    "            \n",
    "        if env[i] < quantile4[i]:\n",
    "            nb_belowq4 += 1\n",
    "            \n",
    "        if env[i] < quantile5[i]:\n",
    "            nb_belowq5 += 1\n",
    "            \n",
    "        if env[i] < quantile6[i]:\n",
    "            nb_belowq6 += 1\n",
    "            \n",
    "        if env[i] < quantile7[i]:\n",
    "            nb_belowq7 += 1\n",
    "            \n",
    "        if env[i] < quantile8[i]:\n",
    "            nb_belowq8 += 1\n",
    "            \n",
    "        if env[i] < quantile9[i]:\n",
    "            nb_belowq9 += 1\n",
    "            \n",
    "        if env[i] < quantile10[i]:\n",
    "            nb_belowq10 += 1\n",
    "\n",
    "        nb_belowq1_list.append(nb_belowq1/(i+1))\n",
    "        nb_belowq2_list.append(nb_belowq2/(i+1))\n",
    "        nb_belowq3_list.append(nb_belowq3/(i+1))\n",
    "        nb_belowq4_list.append(nb_belowq4/(i+1))\n",
    "        nb_belowq5_list.append(nb_belowq5/(i+1))\n",
    "        nb_belowq6_list.append(nb_belowq6/(i+1))\n",
    "        nb_belowq7_list.append(nb_belowq7/(i+1))\n",
    "        nb_belowq8_list.append(nb_belowq8/(i+1))\n",
    "        nb_belowq9_list.append(nb_belowq9/(i+1))\n",
    "        nb_belowq10_list.append(nb_belowq10/(i+1))\n",
    "        \n",
    "        \n",
    "\n",
    "    return nb_belowq1_list, nb_belowq2_list, nb_belowq3_list, nb_belowq4_list, nb_belowq5_list, nb_belowq6_list, nb_belowq7_list, nb_belowq8_list, nb_belowq9_list, nb_belowq10_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acrobot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When taking a step in the env, the state variables are $\\cos{(\\theta_1)}, \\sin{(\\theta_1)}, \\cos{(\\theta_2)}, \\sin{(\\theta_2)}, \\omega_1, \\omega_2$.\n",
    "\n",
    "When using env.state, we have $\\theta_1, \\theta_2, \\omega_1, \\omega_2$. This allows us to have more precision in the angles which is needed in the MPC cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying different PF and pre-trained using rnd actions QRNN - Feb 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train on randomly sampled actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_magnitudes_costheta1 = []\n",
    "error_magnitudes_sintheta1 = []\n",
    "error_magnitudes_costheta2 = []\n",
    "error_magnitudes_sintheta2 = []\n",
    "error_magnitudes_omega1 = []\n",
    "error_magnitudes_omega2 = []\n",
    "\n",
    "env_costheta1 = []\n",
    "env_sintheta1 = []\n",
    "env_costheta2 = []\n",
    "env_sintheta2 = []\n",
    "env_omega1 = []\n",
    "env_omega2 = []\n",
    "\n",
    "pred_costheta1 = []\n",
    "pred_sintheta1 = []\n",
    "pred_costheta2 = []\n",
    "pred_sintheta2 = []\n",
    "pred_omega1 = []\n",
    "pred_omega2 = []\n",
    "\n",
    "# pred_x_var = []\n",
    "# pred_y_var = []\n",
    "# pred_z_var = []\n",
    "\n",
    "seed = 0\n",
    "\n",
    "env = gym.make('Acrobot-v1')#.unwrapped\n",
    "sim_env = gym.make('Acrobot-v1')#.unwrapped  # Additional simulation model for MPC\n",
    "# max_episode_steps = 500\n",
    "# env = gym.make('CartPole-v1').unwrapped\n",
    "# sim_env = gym.make('CartPole-v1').unwrapped  # Additional simulation model for MPC\n",
    "sim_env.reset(seed=seed)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "# Hyperparameters\n",
    "state_dim = env.observation_space.shape[0]#-2#-1 # Since we only care about angle and omega which are given using env.state\n",
    "# action_dim = env.action_space.shape[0]  # For Pendulum, it's continuous\n",
    "action_dim=1\n",
    "\n",
    "# Initialize the Next-State Prediction Network\n",
    "model = NextStateMedianNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_func = quantile_loss_median\n",
    "\n",
    "states_low = torch.tensor([-1, -1, -1, -1, -12.566371, -28.274334])\n",
    "states_high = torch.tensor([1, 1, 1, 1, 12.566371, 28.274334])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Experience replay buffer\n",
    "replay_buffer = []\n",
    "discrete = True\n",
    "num_test_steps = 20000\n",
    "\n",
    "state, _ = env.reset(seed=seed)\n",
    "episode_reward = 0\n",
    "actions_list = []\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "actions_taken = np.zeros(num_test_steps)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "for step in range(num_test_steps):\n",
    "    # state = env.state\n",
    "    action = env.action_space.sample()\n",
    "    actions_taken[step] = action\n",
    "    \n",
    "    # Apply the first action from the optimized sequence\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    # next_state = env.state\n",
    "    episode_reward += reward\n",
    "    actions_list.append(action)\n",
    "    \n",
    "    if step >= 1:\n",
    "        # test_env = env.copy()\n",
    "        # test_action = test_env.action_space.sample()\n",
    "        # test_next_state, _, _, _, _ = test_env.step(test_action)\n",
    "        # test_next_state = test_next_state['observation'][:3]\n",
    "        \n",
    "        test_next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        # test_next_state = torch.clip(test_next_state, states_low, states_high)\n",
    "        # print(\"test_next_state \", test_next_state, \"\\n\")\n",
    "        \n",
    "        if discrete:\n",
    "            test_action = torch.tensor(np.array([action]), dtype=torch.float32)#.unsqueeze(1)  # Example action\n",
    "        else:\n",
    "            test_action = torch.tensor(action, dtype=torch.float32)\n",
    "            \n",
    "        test_state = torch.tensor(state, dtype=torch.float32)#.reshape(1, -1)\n",
    "        test_state = torch.clip(test_state, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        preds = model(test_state, test_action)  # Shape: (1, num_quantiles, state_dim)\n",
    "        # lower_quantile = predicted_quantiles[:, 0, :]  # Shape: (1, state_dim)\n",
    "        # mid_quantile = predicted_quantiles[:, int(num_quantiles/2), :].detach().numpy()  # Shape: (1, state_dim)\n",
    "        # upper_quantile = predicted_quantiles[:, -1, :]  # Shape: (1, state_dim)\n",
    "        # print(\"predicted_quantiles \", predicted_quantiles, \"\\n\")\n",
    "        # print(\"Lower Quantile:\", lower_quantile, \"\\n\")\n",
    "        # print(\"Mid Quantile:\", mid_quantile, \"\\n\")\n",
    "        # print(\"Upper Quantile:\", upper_quantile, \"\\n\")\n",
    "        \n",
    "        pred_costheta1 = np.append(pred_costheta1, preds[0].detach().numpy())\n",
    "        pred_sintheta1 = np.append(pred_sintheta1, preds[1].detach().numpy())\n",
    "        pred_costheta2 = np.append(pred_costheta2, preds[2].detach().numpy()) \n",
    "        pred_sintheta2 = np.append(pred_sintheta2, preds[3].detach().numpy())\n",
    "        # pred_theta2 = np.append(pred_sintheta2, preds[3].detach().numpy())  \n",
    "        pred_omega1 = np.append(pred_omega1, preds[4].detach().numpy())\n",
    "        pred_omega2 = np.append(pred_omega2, preds[5].detach().numpy())\n",
    "        \n",
    "        deltacostheta1 = test_next_state[0] - preds[0]\n",
    "        deltasintheta1 = test_next_state[1] - preds[1]\n",
    "        deltacostheta2 = test_next_state[2] - preds[2]\n",
    "        deltasintheta2 = test_next_state[3] - preds[3]\n",
    "        deltaomega1 = test_next_state[4] - preds[4]\n",
    "        deltaomega2 = test_next_state[5] - preds[5]\n",
    "\n",
    "        # delta = env_test_state - mean_next_state.flatten()#.detach().numpy()\n",
    "        # print(\"delta \", delta, \"\\n\") # 6\n",
    "        # print(\"delta_x \", deltax, \"\\n\") # 7\n",
    "        # error_magnitude = np.linalg.norm(delta)\n",
    "        # error_magnitudes.append(error_magnitude)\n",
    "        error_magnitudes_costheta1.append(np.abs(deltacostheta1.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_sintheta1.append(np.abs(deltasintheta1.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_costheta2.append(np.abs(deltacostheta2.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_sintheta2.append(np.abs(deltasintheta2.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_omega1.append(np.abs(deltaomega1.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_omega2.append(np.abs(deltaomega2.detach().numpy())) # .detach().numpy()\n",
    "        # env_next_states.append(env_test_state)\n",
    "        env_costheta1.append(test_next_state[0])\n",
    "        env_sintheta1.append(test_next_state[1])\n",
    "        env_costheta2.append(test_next_state[2])\n",
    "        env_sintheta2.append(test_next_state[3])\n",
    "        env_omega1.append(test_next_state[4])\n",
    "        env_omega2.append(test_next_state[5])\n",
    "    \n",
    "    # next_state = env.state.copy()\n",
    "    # Store experience in replay buffer\n",
    "    replay_buffer.append((state, np.array([action]), reward, next_state, done))\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        pass\n",
    "    else:\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "        \n",
    "        states = torch.clip(states, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        predicted_quantiles = model(states, actions)  # Shape: (batch_size, num_quantiles, state_dim)\n",
    "        \n",
    "        # Use next state as target (can be improved with target policy)\n",
    "        target_quantiles = next_states\n",
    "        \n",
    "        # Compute the target quantiles (e.g., replicate next state across the quantile dimension)\n",
    "        # target_quantiles = next_states.unsqueeze(-1).repeat(1, 1, num_quantiles)\n",
    "\n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "\n",
    "        loss = loss_func(predicted_quantiles, target_quantiles)\n",
    "        \n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "        \n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    state = next_state\n",
    "    \n",
    "    if done or truncated:\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        episode_reward = 0\n",
    "        actions_list = []\n",
    "        done = False\n",
    "        truncated = False\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 1., ..., 2., 2., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions_taken\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for the acrobot env when using cos(theta1), sin(theta1), cos(theta2), sin(theta2), omega1, omega2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.seed(seed)\n",
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.014046903 0.012131164 0.015980767 0.015819404 0.021358531 0.031639658\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(error_magnitudes_costheta1),\n",
    "      np.mean(error_magnitudes_sintheta1),\n",
    "      np.mean(error_magnitudes_costheta2),\n",
    "      np.mean(error_magnitudes_sintheta2),\n",
    "      np.mean(error_magnitudes_omega1),\n",
    "      np.mean(error_magnitudes_omega2),)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011049359 0.010617394 0.014177004 0.014911825 0.019920189 0.030221928\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(error_magnitudes_costheta1[-100:]),\n",
    "      np.mean(error_magnitudes_sintheta1[-100:]),\n",
    "      np.mean(error_magnitudes_costheta2[-100:]),\n",
    "      np.mean(error_magnitudes_sintheta2[-100:]),\n",
    "      np.mean(error_magnitudes_omega1[-100:]),\n",
    "      np.mean(error_magnitudes_omega2[-100:]),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All steps\n",
      "$\\cos(\\theta_1)$ comparison: 0.014046902393641541\n",
      "$\\sin(\\theta_1)$ comparison: 0.012131163737253772\n",
      "$\\cos(\\theta_2)$ comparison: 0.015980767850582948\n",
      "$\\sin(\\theta_2)$ comparison: 0.015819404716732904\n",
      "$(\\omega_1)$ comparison: 0.02135853185516712\n",
      "$(\\omega_2)$ comparison: 0.031639661073600896\n"
     ]
    }
   ],
   "source": [
    "print(\"All steps\")\n",
    "print(f\"$\\\\cos(\\\\theta_1)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_costheta1)-np.array(env_costheta1))))\n",
    "\n",
    "print(f\"$\\\\sin(\\\\theta_1)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_sintheta1)-np.array(env_sintheta1))))\n",
    "\n",
    "print(f\"$\\\\cos(\\\\theta_2)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_costheta2)-np.array(env_costheta2))))\n",
    "\n",
    "print(f\"$\\\\sin(\\\\theta_2)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_sintheta2)-np.array(env_sintheta2))))\n",
    "\n",
    "print(f\"$(\\\\omega_1)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega1)-np.array(env_omega1))))\n",
    "\n",
    "print(f\"$(\\\\omega_2)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega2)-np.array(env_omega2))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amonsgt last 100 steps:\n",
      "$\\cos(\\theta_1)$ comparison: 0.011049357652664184\n",
      "$\\sin(\\theta_1)$ comparison: 0.010617395592853428\n",
      "$\\cos(\\theta_2)$ comparison: 0.014177004601806402\n",
      "$\\sin(\\theta_2)$ comparison: 0.01491182418394601\n",
      "$(\\omega_1)$ comparison: 0.019920188840478657\n",
      "$(\\omega_2)$ comparison: 0.030221926495432854\n"
     ]
    }
   ],
   "source": [
    "print(\"Amonsgt last 100 steps:\")\n",
    "print(f\"$\\\\cos(\\\\theta_1)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_costheta1[-100:])-np.array(env_costheta1[-100:]))))\n",
    "\n",
    "print(f\"$\\\\sin(\\\\theta_1)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_sintheta1[-100:])-np.array(env_sintheta1[-100:]))))\n",
    "\n",
    "print(f\"$\\\\cos(\\\\theta_2)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_costheta2[-100:])-np.array(env_costheta2[-100:]))))\n",
    "\n",
    "print(f\"$\\\\sin(\\\\theta_2)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_sintheta2[-100:])-np.array(env_sintheta2[-100:]))))\n",
    "\n",
    "print(f\"$(\\\\omega_1)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega1[-100:])-np.array(env_omega1[-100:]))))\n",
    "\n",
    "print(f\"$(\\\\omega_2)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega2[-100:])-np.array(env_omega2[-100:]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amonsgt last 2000 steps/Without first 18000 steps:\n",
      "$\\cos(\\theta_1)$ comparison: 0.009719800132640328\n",
      "$\\sin(\\theta_1)$ comparison: 0.009439033125026948\n",
      "$\\cos(\\theta_2)$ comparison: 0.01079608149642615\n",
      "$\\sin(\\theta_2)$ comparison: 0.011422194447918721\n",
      "$(\\omega_1)$ comparison: 0.014890437378497302\n",
      "$(\\omega_2)$ comparison: 0.022839846410075776\n"
     ]
    }
   ],
   "source": [
    "print(\"Amonsgt last 2000 steps/Without first 18000 steps:\")\n",
    "print(f\"$\\\\cos(\\\\theta_1)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_costheta1[18000:])-np.array(env_costheta1[18000:]))))\n",
    "\n",
    "print(f\"$\\\\sin(\\\\theta_1)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_sintheta1[18000:])-np.array(env_sintheta1[18000:]))))\n",
    "\n",
    "print(f\"$\\\\cos(\\\\theta_2)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_costheta2[18000:])-np.array(env_costheta2[18000:]))))\n",
    "\n",
    "print(f\"$\\\\sin(\\\\theta_2)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_sintheta2[18000:])-np.array(env_sintheta2[18000:]))))\n",
    "\n",
    "print(f\"$(\\\\omega_1)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega1[18000:])-np.array(env_omega1[18000:]))))\n",
    "\n",
    "print(f\"$(\\\\omega_2)$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega2[18000:])-np.array(env_omega2[18000:]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(1)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_costheta1)\n",
    "# plt.ylabel(f'Error magnitudes $\\\\cos(\\\\theta_1)$')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_1_{timestamp}.png')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# plt.figure(2)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_sintheta1)\n",
    "# plt.ylabel(f'Error magnitudes $\\\\sin(\\\\theta_1)$')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_1_{timestamp}.png')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# plt.figure(3)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_costheta2)\n",
    "# plt.ylabel(f'Error magnitudes $\\\\cos(\\\\theta_1)$')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_2_{timestamp}.png')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# plt.figure(4)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_sintheta2)\n",
    "# plt.ylabel(f'Error magnitudes $\\\\sin(\\\\theta_2)$')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_2_{timestamp}.png')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# plt.figure(5)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_omega1)\n",
    "# plt.ylabel(f'Error magnitudes $\\omega_1$')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_1_{timestamp}.png')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# plt.figure(6)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_omega2)\n",
    "# plt.ylabel(f'Error magnitudes $\\omega_2$')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_2_{timestamp}.png')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# plt.figure(7)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel(f'$\\\\cos(\\\\theta_1)$ comparison between env and QRNN')\n",
    "# plt.plot(env_costheta1, label='env_next_states')\n",
    "# plt.plot(pred_costheta1, label='pred_next_states')\n",
    "# # plt.fill_between(np.arange(len(pred_theta1)), pred_theta1 - pred_theta1_bottom_1std, pred_theta1 + pred_theta1_top_2std, alpha=0.5, label='1 std', color='green')\n",
    "# # plt.fill_between(np.arange(len(pred_theta1)), pred_theta1 - pred_theta1_bottom_2std, pred_theta1 + pred_theta1_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\angles_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure(8)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel(f'$\\\\sin(\\\\theta_1)$ comparison between env and QRNN')\n",
    "# plt.plot(env_sintheta1, label='env_next_states')\n",
    "# plt.plot(pred_sintheta1, label='pred_next_states')\n",
    "# # plt.fill_between(np.arange(len(pred_theta1)), pred_theta1 - pred_theta1_bottom_1std, pred_theta1 + pred_theta1_top_2std, alpha=0.5, label='1 std', color='green')\n",
    "# # plt.fill_between(np.arange(len(pred_theta1)), pred_theta1 - pred_theta1_bottom_2std, pred_theta1 + pred_theta1_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\angles_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure(9)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel(f'$\\\\cos(\\\\theta_2)$ comparison between env and QRNN')\n",
    "# plt.plot(env_costheta2, label='env_next_states')\n",
    "# plt.plot(pred_costheta2, label='pred_next_states')\n",
    "# # plt.fill_between(np.arange(len(pred_theta2)), pred_theta2 - pred_theta2_bottom_1std, pred_theta2 + pred_theta2_top_2std, alpha=0.5, label='1 std', color='green')\n",
    "# # plt.fill_between(np.arange(len(pred_theta2)), pred_theta2 - pred_theta2_bottom_2std, pred_theta2 + pred_theta2_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\omegas_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure(10)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel(f'$\\\\sin(\\\\theta_2)$ comparison between env and QRNN')\n",
    "# plt.plot(env_sintheta2, label='env_next_states')\n",
    "# plt.plot(pred_sintheta2, label='pred_next_states')\n",
    "# # plt.fill_between(np.arange(len(pred_theta2)), pred_theta2 - pred_theta2_bottom_1std, pred_theta2 + pred_theta2_top_2std, alpha=0.5, label='1 std', color='green')\n",
    "# # plt.fill_between(np.arange(len(pred_theta2)), pred_theta2 - pred_theta2_bottom_2std, pred_theta2 + pred_theta2_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\omegas_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure(11)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel(f'$\\omega_1$ comparison between env and QRNN')\n",
    "# plt.plot(env_omega1, label='env_next_states')\n",
    "# plt.plot(pred_omega1, label='pred_next_states')\n",
    "# # plt.fill_between(np.arange(len(pred_omega1)), pred_omega1 - pred_omega1_bottom_1std, pred_omega1 + pred_omega1_top_1std, alpha=0.5, label='1 std', color='green')\n",
    "# # plt.fill_between(np.arange(len(pred_omega1)), pred_omega1 - pred_omega1_bottom_2std, pred_omega1 + pred_omega1_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\angles_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure(12)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel(f'$\\omega_2$ comparison between env and QRNN')\n",
    "# plt.plot(env_omega2, label='env_next_states')\n",
    "# plt.plot(pred_omega2, label='pred_next_states')\n",
    "# # plt.fill_between(np.arange(len(pred_omega2)), pred_omega2 - pred_omega2_bottom_1std, pred_omega2 + pred_omega2_top_1std, alpha=0.5, label='1 std', color='green')\n",
    "# # plt.fill_between(np.arange(len(pred_omega2)), pred_omega2 - pred_omega2_bottom_2std, pred_omega2 + pred_omega2_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\omegas_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cart pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_magnitudes_x = []\n",
    "error_magnitudes_v = []\n",
    "error_magnitudes_theta = []\n",
    "error_magnitudes_omega = []\n",
    "\n",
    "env_x = []\n",
    "env_v = []\n",
    "env_theta = []\n",
    "env_omega = []\n",
    "\n",
    "pred_x = []\n",
    "pred_v = []\n",
    "pred_theta = []\n",
    "pred_omega = []\n",
    "\n",
    "# pred_x_var = []\n",
    "# pred_y_var = []\n",
    "# pred_z_var = []\n",
    "\n",
    "seed = 0\n",
    "\n",
    "env = gym.make('CartPole-v1')#.unwrapped\n",
    "sim_env = gym.make('CartPole-v1')#.unwrapped  # Additional simulation model for MPC\n",
    "# max_episode_steps = 500\n",
    "# env = gym.make('CartPole-v1').unwrapped\n",
    "# sim_env = gym.make('CartPole-v1').unwrapped  # Additional simulation model for MPC\n",
    "sim_env.reset(seed=seed)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "# Hyperparameters\n",
    "state_dim = env.observation_space.shape[0]#-2#-1 # Since we only care about angle and omega which are given using env.state\n",
    "# action_dim = env.action_space.shape[0]  # For Pendulum, it's continuous\n",
    "action_dim=1\n",
    "\n",
    "# Initialize the Next-State Prediction Network\n",
    "model = NextStateMedianNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_func = quantile_loss_median\n",
    "\n",
    "states_low = torch.tensor([-4.8, -torch.inf, -0.41887903, -torch.inf])\n",
    "states_high = torch.tensor([4.8, torch.inf, 0.41887903, torch.inf])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Experience replay buffer\n",
    "replay_buffer = []\n",
    "discrete = True\n",
    "num_test_steps = 20000\n",
    "\n",
    "state, _ = env.reset(seed=seed)\n",
    "episode_reward = 0\n",
    "actions_list = []\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "actions_taken = np.zeros(num_test_steps)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "for step in range(num_test_steps):\n",
    "    # state = env.state\n",
    "    action = env.action_space.sample()\n",
    "    actions_taken[step] = action\n",
    "    \n",
    "    # Apply the first action from the optimized sequence\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    # next_state = env.state\n",
    "    episode_reward += reward\n",
    "    actions_list.append(action)\n",
    "    \n",
    "    if step >= 1:\n",
    "        # test_env = env.copy()\n",
    "        # test_action = test_env.action_space.sample()\n",
    "        # test_next_state, _, _, _, _ = test_env.step(test_action)\n",
    "        # test_next_state = test_next_state['observation'][:3]\n",
    "        \n",
    "        test_next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        # test_next_state = torch.clip(test_next_state, states_low, states_high)\n",
    "        # print(\"test_next_state \", test_next_state, \"\\n\")\n",
    "        \n",
    "        if discrete:\n",
    "            test_action = torch.tensor(np.array([action]), dtype=torch.float32)#.unsqueeze(1)  # Example action\n",
    "        else:\n",
    "            test_action = torch.tensor(np.array([action]), dtype=torch.float32).unsqueeze(1)\n",
    "            \n",
    "        test_state = torch.tensor(state, dtype=torch.float32)#.reshape(1, -1)\n",
    "        test_state = torch.clip(test_state, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        preds = model(test_state, test_action)  # Shape: (1, num_quantiles, state_dim)\n",
    "        # lower_quantile = predicted_quantiles[:, 0, :]  # Shape: (1, state_dim)\n",
    "        # mid_quantile = predicted_quantiles[:, int(num_quantiles/2), :].detach().numpy()  # Shape: (1, state_dim)\n",
    "        # upper_quantile = predicted_quantiles[:, -1, :]  # Shape: (1, state_dim)\n",
    "        # print(\"predicted_quantiles \", predicted_quantiles, \"\\n\")\n",
    "        # print(\"Lower Quantile:\", lower_quantile, \"\\n\")\n",
    "        # print(\"Mid Quantile:\", mid_quantile, \"\\n\")\n",
    "        # print(\"Upper Quantile:\", upper_quantile, \"\\n\")\n",
    "        \n",
    "        pred_x = np.append(pred_x, preds[0].detach().numpy())\n",
    "        pred_v = np.append(pred_v, preds[1].detach().numpy())\n",
    "        pred_theta = np.append(pred_theta, preds[2].detach().numpy()) \n",
    "        pred_omega = np.append(pred_omega, preds[3].detach().numpy())\n",
    "        # # pred_theta2 = np.append(pred_sintheta2, preds[3].detach().numpy())  \n",
    "        # pred_omega1 = np.append(pred_omega1, preds[4].detach().numpy())\n",
    "        # pred_omega2 = np.append(pred_omega2, preds[5].detach().numpy())\n",
    "        \n",
    "        deltax = test_next_state[0] - preds[0]\n",
    "        deltav = test_next_state[1] - preds[1]\n",
    "        deltatheta = test_next_state[2] - preds[2]\n",
    "        deltaomega = test_next_state[3] - preds[3]\n",
    "        # deltaomega1 = test_next_state[4] - preds[4]\n",
    "        # deltaomega2 = test_next_state[5] - preds[5]\n",
    "\n",
    "        # delta = env_test_state - mean_next_state.flatten()#.detach().numpy()\n",
    "        # print(\"delta \", delta, \"\\n\") # 6\n",
    "        # print(\"delta_x \", deltax, \"\\n\") # 7\n",
    "        # error_magnitude = np.linalg.norm(delta)\n",
    "        # error_magnitudes.append(error_magnitude)\n",
    "        error_magnitudes_x.append(np.abs(deltax.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_v.append(np.abs(deltav.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_theta.append(np.abs(deltatheta.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_omega.append(np.abs(deltaomega.detach().numpy())) # .detach().numpy()\n",
    "        # error_magnitudes_omega1.append(np.abs(deltaomega1.detach().numpy())) # .detach().numpy()\n",
    "        # error_magnitudes_omega2.append(np.abs(deltaomega2.detach().numpy())) # .detach().numpy()\n",
    "        # env_next_states.append(env_test_state)\n",
    "        env_x.append(test_next_state[0])\n",
    "        env_v.append(test_next_state[1])\n",
    "        env_theta.append(test_next_state[2])\n",
    "        env_omega.append(test_next_state[3])\n",
    "        # env_omega1.append(test_next_state[4])\n",
    "        # env_omega2.append(test_next_state[5])\n",
    "    \n",
    "    # next_state = env.state.copy()\n",
    "    # Store experience in replay buffer\n",
    "    if discrete:\n",
    "        replay_buffer.append((state, np.array([action]), reward, next_state, done))\n",
    "    else:\n",
    "        replay_buffer.append((state, np.array(action), reward, next_state, done))\n",
    "        \n",
    "    if len(replay_buffer) < batch_size:\n",
    "        pass\n",
    "    else:\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "        \n",
    "        states = torch.clip(states, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        predicted_quantiles = model(states, actions)  # Shape: (batch_size, num_quantiles, state_dim)\n",
    "        \n",
    "        # Use next state as target (can be improved with target policy)\n",
    "        target_quantiles = next_states\n",
    "        \n",
    "        # Compute the target quantiles (e.g., replicate next state across the quantile dimension)\n",
    "        # target_quantiles = next_states.unsqueeze(-1).repeat(1, 1, num_quantiles)\n",
    "\n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "\n",
    "        loss = loss_func(predicted_quantiles, target_quantiles)\n",
    "        \n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "        \n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    state = next_state\n",
    "    \n",
    "    if done or truncated:\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        episode_reward = 0\n",
    "        actions_list = []\n",
    "        done = False\n",
    "        truncated = False\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 steps\n",
      "x comparison: 0.0014790624752640724\n",
      "v comparison: 0.0027963328547775746\n",
      "$\\theta$ comparison: 0.0023765639240446034\n",
      "$\\omega$ comparison: 0.003939865119755268\n"
     ]
    }
   ],
   "source": [
    "print(\"Last 100 steps\")\n",
    "print(f\"x comparison:\",\n",
    "np.mean(np.abs(np.array(pred_x[-100:])-np.array(env_x[-100:]))))\n",
    "\n",
    "print(f\"v comparison:\",\n",
    "np.mean(np.abs(np.array(pred_v[-100:])-np.array(env_v[-100:]))))\n",
    "\n",
    "print(f\"$\\\\theta$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_theta[-100:])-np.array(env_theta[-100:]))))\n",
    "\n",
    "print(f\"$\\\\omega$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega[-100:])-np.array(env_omega[-100:]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All steps\n",
      "x comparison: 0.0036918997990464794\n",
      "v comparison: 0.006502414954312139\n",
      "$\\theta$ comparison: 0.0039861937003700465\n",
      "$\\omega$ comparison: 0.007583585796413758\n"
     ]
    }
   ],
   "source": [
    "print(\"All steps\")\n",
    "print(f\"x comparison:\",\n",
    "np.mean(np.abs(np.array(pred_x)-np.array(env_x))))\n",
    "\n",
    "print(f\"v comparison:\",\n",
    "np.mean(np.abs(np.array(pred_v)-np.array(env_v))))\n",
    "\n",
    "print(f\"$\\\\theta$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_theta)-np.array(env_theta))))\n",
    "\n",
    "print(f\"$\\\\omega$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega)-np.array(env_omega))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for cart pole env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot training results\n",
    "# import matplotlib.pyplot as plt\n",
    "# # Plot the rewards\n",
    "# plt.figure(1)\n",
    "# # plt.legend()\n",
    "# # plt.grid()\n",
    "# plt.plot(episode_reward_list)\n",
    "# plt.xlabel('Nb of episodes')\n",
    "# plt.ylabel('episode reward')\n",
    "# # plt.title('MPC_Pendulum with Optimized Action Sequences')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\Graphs\\\\episode_rewards_withCPUforQRNN_{timestamp}.png')\n",
    "# # # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\episode_rewards_withGPUforGP_{timestamp}.png')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# plt.figure(2)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_x)\n",
    "# plt.ylabel('error_magnitudes_x')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_1_{timestamp}.png')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# plt.figure(3)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_v)\n",
    "# plt.ylabel('error_magnitudes_v')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_2_{timestamp}.png')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# plt.figure(4)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_theta)\n",
    "# plt.ylabel('error_magnitudes_theta')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_1_{timestamp}.png')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# plt.figure(5)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_omega)\n",
    "# plt.ylabel('error_magnitudes_omega')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_2_{timestamp}.png')\n",
    "# plt.yscale('log')\n",
    "\n",
    "# plt.figure(6)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel('x comparison between env and QRNN')\n",
    "# plt.plot(env_x, label='env_next_states')\n",
    "# plt.plot(pred_x, label='pred_next_states')\n",
    "# plt.fill_between(np.arange(len(pred_x)), pred_x - pred_x_bottom_1std, pred_x + pred_x_top_2std, alpha=0.5, label='1 std', color='green')\n",
    "# plt.fill_between(np.arange(len(pred_x)), pred_x - pred_x_bottom_2std, pred_x + pred_x_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\angles_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure(7)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel('v comparison between env and QRNN')\n",
    "# plt.plot(env_v, label='env_next_states')\n",
    "# plt.plot(pred_v, label='pred_next_states')\n",
    "# plt.fill_between(np.arange(len(pred_v)), pred_v - pred_v_bottom_1std, pred_v + pred_v_top_1std, alpha=0.5, label='1 std', color='green')\n",
    "# plt.fill_between(np.arange(len(pred_v)), pred_v - pred_v_bottom_2std, pred_v + pred_v_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\omegas_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure(8)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel('theta comparison between env and QRNN')\n",
    "# plt.plot(env_theta, label='env_next_states')\n",
    "# plt.plot(pred_theta, label='pred_next_states')\n",
    "# plt.fill_between(np.arange(len(pred_theta)), pred_theta - pred_theta_bottom_1std, pred_theta + pred_theta_top_2std, alpha=0.5, label='1 std', color='green')\n",
    "# plt.fill_between(np.arange(len(pred_theta)), pred_theta - pred_theta_bottom_2std, pred_theta + pred_theta_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\angles_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure(9)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel('omega comparison between env and QRNN')\n",
    "# plt.plot(env_omega, label='env_next_states')\n",
    "# plt.plot(pred_omega, label='pred_next_states')\n",
    "# plt.fill_between(np.arange(len(pred_omega)), pred_omega - pred_omega_bottom_1std, pred_omega + pred_omega_top_1std, alpha=0.5, label='1 std', color='green')\n",
    "# plt.fill_between(np.arange(len(pred_omega)), pred_omega - pred_omega_bottom_2std, pred_omega + pred_omega_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\omegas_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pendulum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_magnitudes_x = []\n",
    "error_magnitudes_y = []\n",
    "error_magnitudes_omega = []\n",
    "\n",
    "env_x = []\n",
    "env_y = []\n",
    "env_omega = []\n",
    "\n",
    "pred_x = []\n",
    "pred_y = []\n",
    "pred_omega = []\n",
    "\n",
    "seed = 0\n",
    "discrete = False\n",
    "env = gym.make('Pendulum-v1')#.unwrapped\n",
    "# sim_env = gym.make('Pendulum-v1')#.unwrapped  # Additional simulation model for MPC\n",
    "# max_episode_steps = 500\n",
    "# env = gym.make('CartPole-v1').unwrapped\n",
    "# sim_env = gym.make('CartPole-v1').unwrapped  # Additional simulation model for MPC\n",
    "sim_env.reset(seed=seed)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "# Hyperparameters\n",
    "state_dim = env.observation_space.shape[0]#-2#-1 # Since we only care about angle and omega which are given using env.state\n",
    "# action_dim = env.action_space.shape[0]  # For Pendulum, it's continuous\n",
    "action_dim=1\n",
    "\n",
    "# Initialize the Next-State Prediction Network\n",
    "model = NextStateMedianNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_func = quantile_loss_median\n",
    "\n",
    "states_low = torch.tensor([-1, -1, -8])\n",
    "states_high = torch.tensor([1, 1, 8])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Experience replay buffer\n",
    "replay_buffer = []\n",
    "\n",
    "num_test_steps = 20000\n",
    "\n",
    "state, _ = env.reset(seed=seed)\n",
    "episode_reward = 0\n",
    "actions_list = []\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "actions_taken = np.zeros(num_test_steps)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "for step in range(num_test_steps):\n",
    "    # state = env.state\n",
    "    action = env.action_space.sample()\n",
    "    actions_taken[step] = action\n",
    "    \n",
    "    # Apply the first action from the optimized sequence\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    # next_state = env.state\n",
    "    episode_reward += reward\n",
    "    actions_list.append(action)\n",
    "    \n",
    "    if step >= 1:\n",
    "        # test_env = env.copy()\n",
    "        # test_action = test_env.action_space.sample()\n",
    "        # test_next_state, _, _, _, _ = test_env.step(test_action)\n",
    "        # test_next_state = test_next_state['observation'][:3]\n",
    "        \n",
    "        test_next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        # test_next_state = torch.clip(test_next_state, states_low, states_high)\n",
    "        # print(\"test_next_state \", test_next_state, \"\\n\")\n",
    "        \n",
    "        if discrete:\n",
    "            test_action = torch.tensor(np.array([action]), dtype=torch.float32)#.unsqueeze(1)  # Example action\n",
    "        else:\n",
    "            test_action = torch.tensor(action, dtype=torch.float32)#.unsqueeze(1)\n",
    "        test_state = torch.tensor(state, dtype=torch.float32)#.reshape(1, -1)\n",
    "        test_state = torch.clip(test_state, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        preds = model(test_state, test_action)  # Shape: (1, num_quantiles, state_dim)\n",
    "        # lower_quantile = predicted_quantiles[:, 0, :]  # Shape: (1, state_dim)\n",
    "        # mid_quantile = predicted_quantiles[:, int(num_quantiles/2), :].detach().numpy()  # Shape: (1, state_dim)\n",
    "        # upper_quantile = predicted_quantiles[:, -1, :]  # Shape: (1, state_dim)\n",
    "        # print(\"predicted_quantiles \", predicted_quantiles, \"\\n\")\n",
    "        # print(\"Lower Quantile:\", lower_quantile, \"\\n\")\n",
    "        # print(\"Mid Quantile:\", mid_quantile, \"\\n\")\n",
    "        # print(\"Upper Quantile:\", upper_quantile, \"\\n\")\n",
    "        \n",
    "        pred_x = np.append(pred_x, preds[0].detach().numpy())\n",
    "        pred_y = np.append(pred_y, preds[1].detach().numpy())\n",
    "        pred_omega = np.append(pred_omega, preds[2].detach().numpy())\n",
    "        # # pred_theta2 = np.append(pred_sintheta2, preds[3].detach().numpy())  \n",
    "        # pred_omega1 = np.append(pred_omega1, preds[4].detach().numpy())\n",
    "        # pred_omega2 = np.append(pred_omega2, preds[5].detach().numpy())\n",
    "        \n",
    "        deltax = test_next_state[0] - preds[0]\n",
    "        deltay = test_next_state[1] - preds[1]\n",
    "        deltaomega = test_next_state[2] - preds[2]\n",
    "        # deltaomega = test_next_state[3] - preds[3]\n",
    "        # deltaomega1 = test_next_state[4] - preds[4]\n",
    "        # deltaomega2 = test_next_state[5] - preds[5]\n",
    "\n",
    "        # delta = env_test_state - mean_next_state.flatten()#.detach().numpy()\n",
    "        # print(\"delta \", delta, \"\\n\") # 6\n",
    "        # print(\"delta_x \", deltax, \"\\n\") # 7\n",
    "        # error_magnitude = np.linalg.norm(delta)\n",
    "        # error_magnitudes.append(error_magnitude)\n",
    "        error_magnitudes_x.append(np.abs(deltax.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_y.append(np.abs(deltay.detach().numpy())) # .detach().numpy()\n",
    "        # error_magnitudes_theta.append(np.abs(deltatheta.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_omega.append(np.abs(deltaomega.detach().numpy())) # .detach().numpy()\n",
    "        # error_magnitudes_omega1.append(np.abs(deltaomega1.detach().numpy())) # .detach().numpy()\n",
    "        # error_magnitudes_omega2.append(np.abs(deltaomega2.detach().numpy())) # .detach().numpy()\n",
    "        # env_next_states.append(env_test_state)\n",
    "        env_x.append(test_next_state[0])\n",
    "        env_y.append(test_next_state[1])\n",
    "        env_omega.append(test_next_state[2])\n",
    "        # env_omega.append(test_next_state[3])\n",
    "        # env_omega1.append(test_next_state[4])\n",
    "        # env_omega2.append(test_next_state[5])\n",
    "    \n",
    "    # next_state = env.state.copy()\n",
    "    # Store experience in replay buffer\n",
    "    if discrete:\n",
    "        replay_buffer.append((state, np.array([action]), reward, next_state, done))\n",
    "    else:\n",
    "        replay_buffer.append((state, np.array(action), reward, next_state, done))\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        pass\n",
    "    else:\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "        \n",
    "        states = torch.clip(states, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        predicted_quantiles = model(states, actions)  # Shape: (batch_size, num_quantiles, state_dim)\n",
    "        \n",
    "        # Use next state as target (can be improved with target policy)\n",
    "        target_quantiles = next_states\n",
    "        \n",
    "        # Compute the target quantiles (e.g., replicate next state across the quantile dimension)\n",
    "        # target_quantiles = next_states.unsqueeze(-1).repeat(1, 1, num_quantiles)\n",
    "\n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "\n",
    "        loss = loss_func(predicted_quantiles, target_quantiles)\n",
    "        \n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "        \n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    state = next_state\n",
    "    \n",
    "    if done or truncated:\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        episode_reward = 0\n",
    "        actions_list = []\n",
    "        done = False\n",
    "        truncated = False\n",
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 steps\n",
      "x comparison: 0.008107431288808585\n",
      "y comparison: 0.015759614994749427\n",
      "$\\omega$ comparison: 0.023014833331108094\n"
     ]
    }
   ],
   "source": [
    "print(\"Last 100 steps\")\n",
    "print(f\"x comparison:\",\n",
    "np.mean(np.abs(np.array(pred_x[-100:])-np.array(env_x[-100:]))))\n",
    "\n",
    "print(f\"y comparison:\",\n",
    "np.mean(np.abs(np.array(pred_y[-100:])-np.array(env_y[-100:]))))\n",
    "\n",
    "print(f\"$\\omega$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega[-100:])-np.array(env_omega[-100:]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All steps\n",
      "x comparison: 0.017279714541051934\n",
      "y comparison: 0.018080271490379527\n",
      "$\\omega$ comparison: 0.03348231717623276\n"
     ]
    }
   ],
   "source": [
    "print(\"All steps\")\n",
    "print(f\"x comparison:\",\n",
    "np.mean(np.abs(np.array(pred_x)-np.array(env_x))))\n",
    "\n",
    "print(f\"y comparison:\",\n",
    "np.mean(np.abs(np.array(pred_y)-np.array(env_y))))\n",
    "\n",
    "print(f\"$\\omega$ comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega)-np.array(env_omega))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plots for the pendulum env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # # Plot the rewards\n",
    "# # plt.figure(1)\n",
    "# # # plt.legend()\n",
    "# # # plt.grid()\n",
    "# # plt.plot(episode_reward_list)\n",
    "# # plt.xlabel('Nb of episodes')\n",
    "# # plt.ylabel('episode reward')\n",
    "# # # plt.title('MPC_Pendulum with Optimized Action Sequences')\n",
    "# # # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\Graphs\\\\episode_rewards_withCPUforQRNN_{timestamp}.png')\n",
    "# # # # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\episode_rewards_withGPUforGP_{timestamp}.png')\n",
    "\n",
    "# plt.figure(1)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_x)\n",
    "# plt.ylabel('error_magnitudes_x')\n",
    "# plt.yscale('log')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_1_{timestamp}.png')\n",
    "\n",
    "# plt.figure(2)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_y)\n",
    "# plt.ylabel('error_magnitudes_y')\n",
    "# plt.yscale('log')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_1_{timestamp}.png')\n",
    "\n",
    "# plt.figure(3)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_omega)\n",
    "# plt.ylabel('error_magnitudes_omega')\n",
    "# plt.yscale('log')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_2_{timestamp}.png')\n",
    "\n",
    "\n",
    "# plt.figure(4)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel('x comparison between env and QRNN')\n",
    "# plt.plot(env_x, label='env_next_states')\n",
    "# plt.plot(pred_x, label='pred_next_states')\n",
    "# # plt.fill_between(np.arange(len(pred_theta)), pred_theta - pred_theta_bottom_1std, pred_theta + pred_theta_top_2std, alpha=0.5, label='1 std', color='green')\n",
    "# # plt.fill_between(np.arange(len(pred_theta)), pred_theta - pred_theta_bottom_2std, pred_theta + pred_theta_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\angles_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure(5)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel('y comparison between env and QRNN')\n",
    "# plt.plot(env_y, label='env_next_states')\n",
    "# plt.plot(pred_y, label='pred_next_states')\n",
    "# # plt.fill_between(np.arange(len(pred_theta)), pred_theta - pred_theta_bottom_1std, pred_theta + pred_theta_top_2std, alpha=0.5, label='1 std', color='green')\n",
    "# # plt.fill_between(np.arange(len(pred_theta)), pred_theta - pred_theta_bottom_2std, pred_theta + pred_theta_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\angles_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure(6)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel('omega comparison between env and QRNN')\n",
    "# plt.plot(env_omega, label='env_next_states')\n",
    "# plt.plot(pred_omega, label='pred_next_states')\n",
    "# # plt.fill_between(np.arange(len(pred_omega)), pred_omega - pred_omega_bottom_1std, pred_omega + pred_omega_top_1std, alpha=0.5, label='1 std', color='green')\n",
    "# # plt.fill_between(np.arange(len(pred_omega)), pred_omega - pred_omega_bottom_2std, pred_omega + pred_omega_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\omegas_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mountain car"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_magnitudes_x = []\n",
    "error_magnitudes_v = []\n",
    "\n",
    "env_x = []\n",
    "env_v = []\n",
    "\n",
    "pred_x = []\n",
    "pred_v = []\n",
    "\n",
    "seed = 0\n",
    "discrete = False\n",
    "env = gym.make('MountainCarContinuous-v0')#.unwrapped\n",
    "# sim_env = gym.make('Pendulum-v1')#.unwrapped  # Additional simulation model for MPC\n",
    "# max_episode_steps = 500\n",
    "# env = gym.make('CartPole-v1').unwrapped\n",
    "# sim_env = gym.make('CartPole-v1').unwrapped  # Additional simulation model for MPC\n",
    "sim_env.reset(seed=seed)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "# Hyperparameters\n",
    "state_dim = env.observation_space.shape[0]#-2#-1 # Since we only care about angle and omega which are given using env.state\n",
    "# action_dim = env.action_space.shape[0]  # For Pendulum, it's continuous\n",
    "action_dim=1\n",
    "\n",
    "# Initialize the Next-State Prediction Network\n",
    "model = NextStateMedianNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_func = quantile_loss_median\n",
    "\n",
    "states_low = torch.tensor([-1.2, -0.07])\n",
    "states_high = torch.tensor([0.6, 0.07])\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Experience replay buffer\n",
    "replay_buffer = []\n",
    "\n",
    "num_test_steps = 20000\n",
    "\n",
    "state, _ = env.reset(seed=seed)\n",
    "episode_reward = 0\n",
    "actions_list = []\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "actions_taken = np.zeros(num_test_steps)\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "for step in range(num_test_steps):\n",
    "    # state = env.state\n",
    "    action = env.action_space.sample()\n",
    "    actions_taken[step] = action\n",
    "    \n",
    "    # Apply the first action from the optimized sequence\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "    # next_state = env.state\n",
    "    episode_reward += reward\n",
    "    actions_list.append(action)\n",
    "    \n",
    "    if step >= 1:\n",
    "        # test_env = env.copy()\n",
    "        # test_action = test_env.action_space.sample()\n",
    "        # test_next_state, _, _, _, _ = test_env.step(test_action)\n",
    "        # test_next_state = test_next_state['observation'][:3]\n",
    "        \n",
    "        test_next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        # test_next_state = torch.clip(test_next_state, states_low, states_high)\n",
    "        # print(\"test_next_state \", test_next_state, \"\\n\")\n",
    "        \n",
    "        if discrete:\n",
    "            test_action = torch.tensor(np.array([action]), dtype=torch.float32)#.unsqueeze(1)  # Example action\n",
    "        else:\n",
    "            test_action = torch.tensor(action, dtype=torch.float32)#.unsqueeze(1)\n",
    "        test_state = torch.tensor(state, dtype=torch.float32)#.reshape(1, -1)\n",
    "        test_state = torch.clip(test_state, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        preds = model(test_state, test_action)  # Shape: (1, num_quantiles, state_dim)\n",
    "        # lower_quantile = predicted_quantiles[:, 0, :]  # Shape: (1, state_dim)\n",
    "        # mid_quantile = predicted_quantiles[:, int(num_quantiles/2), :].detach().numpy()  # Shape: (1, state_dim)\n",
    "        # upper_quantile = predicted_quantiles[:, -1, :]  # Shape: (1, state_dim)\n",
    "        # print(\"predicted_quantiles \", predicted_quantiles, \"\\n\")\n",
    "        # print(\"Lower Quantile:\", lower_quantile, \"\\n\")\n",
    "        # print(\"Mid Quantile:\", mid_quantile, \"\\n\")\n",
    "        # print(\"Upper Quantile:\", upper_quantile, \"\\n\")\n",
    "        \n",
    "        pred_x = np.append(pred_x, preds[0].detach().numpy())\n",
    "        pred_v = np.append(pred_v, preds[1].detach().numpy())\n",
    "        # pred_omega = np.append(pred_omega, preds[2].detach().numpy())\n",
    "        # # pred_theta2 = np.append(pred_sintheta2, preds[3].detach().numpy())  \n",
    "        # pred_omega1 = np.append(pred_omega1, preds[4].detach().numpy())\n",
    "        # pred_omega2 = np.append(pred_omega2, preds[5].detach().numpy())\n",
    "        \n",
    "        deltax = test_next_state[0] - preds[0]\n",
    "        deltav = test_next_state[1] - preds[1]\n",
    "        # deltaomega = test_next_state[2] - preds[2]\n",
    "        # deltaomega = test_next_state[3] - preds[3]\n",
    "        # deltaomega1 = test_next_state[4] - preds[4]\n",
    "        # deltaomega2 = test_next_state[5] - preds[5]\n",
    "\n",
    "        # delta = env_test_state - mean_next_state.flatten()#.detach().numpy()\n",
    "        # print(\"delta \", delta, \"\\n\") # 6\n",
    "        # print(\"delta_x \", deltax, \"\\n\") # 7\n",
    "        # error_magnitude = np.linalg.norm(delta)\n",
    "        # error_magnitudes.append(error_magnitude)\n",
    "        error_magnitudes_x.append(np.abs(deltax.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_v.append(np.abs(deltav.detach().numpy())) # .detach().numpy()\n",
    "        # error_magnitudes_theta.append(np.abs(deltatheta.detach().numpy())) # .detach().numpy()\n",
    "        # error_magnitudes_omega.append(np.abs(deltaomega.detach().numpy())) # .detach().numpy()\n",
    "        # error_magnitudes_omega1.append(np.abs(deltaomega1.detach().numpy())) # .detach().numpy()\n",
    "        # error_magnitudes_omega2.append(np.abs(deltaomega2.detach().numpy())) # .detach().numpy()\n",
    "        # env_next_states.append(env_test_state)\n",
    "        env_x.append(test_next_state[0])\n",
    "        env_v.append(test_next_state[1])\n",
    "        # env_omega.append(test_next_state[2])\n",
    "        # env_omega.append(test_next_state[3])\n",
    "        # env_omega1.append(test_next_state[4])\n",
    "        # env_omega2.append(test_next_state[5])\n",
    "    \n",
    "    # next_state = env.state.copy()\n",
    "    # Store experience in replay buffer\n",
    "    if discrete:\n",
    "        replay_buffer.append((state, np.array([action]), reward, next_state, done))\n",
    "    else:\n",
    "        replay_buffer.append((state, np.array(action), reward, next_state, done))\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        pass\n",
    "    else:\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "        \n",
    "        states = torch.clip(states, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        predicted_quantiles = model(states, actions)  # Shape: (batch_size, num_quantiles, state_dim)\n",
    "        \n",
    "        # Use next state as target (can be improved with target policy)\n",
    "        target_quantiles = next_states\n",
    "        \n",
    "        # Compute the target quantiles (e.g., replicate next state across the quantile dimension)\n",
    "        # target_quantiles = next_states.unsqueeze(-1).repeat(1, 1, num_quantiles)\n",
    "\n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "\n",
    "        loss = loss_func(predicted_quantiles, target_quantiles)\n",
    "        \n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "        \n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    state = next_state\n",
    "    \n",
    "    if done or truncated:\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        episode_reward = 0\n",
    "        actions_list = []\n",
    "        done = False\n",
    "        truncated = False\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 steps\n",
      "x comparison: 0.0034873494035855403\n",
      "v comparison: 0.0008802143813227304\n"
     ]
    }
   ],
   "source": [
    "print(\"Last 100 steps\")\n",
    "print(f\"x comparison:\",\n",
    "np.mean(np.abs(np.array(pred_x[-100:])-np.array(env_x[-100:]))))\n",
    "\n",
    "print(f\"v comparison:\",\n",
    "np.mean(np.abs(np.array(pred_v[-100:])-np.array(env_v[-100:]))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All steps\n",
      "x comparison: 0.004670445118318934\n",
      "v comparison: 0.0019249889209457898\n"
     ]
    }
   ],
   "source": [
    "print(\"All steps\")\n",
    "print(f\"x comparison:\",\n",
    "np.mean(np.abs(np.array(pred_x)-np.array(env_x))))\n",
    "\n",
    "print(f\"v comparison:\",\n",
    "np.mean(np.abs(np.array(pred_v)-np.array(env_v))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# # # Plot the rewards\n",
    "# # plt.figure(1)\n",
    "# # # plt.legend()\n",
    "# # # plt.grid()\n",
    "# # plt.plot(episode_reward_list)\n",
    "# # plt.xlabel('Nb of episodes')\n",
    "# # plt.ylabel('episode reward')\n",
    "# # # plt.title('MPC_Pendulum with Optimized Action Sequences')\n",
    "# # # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\Graphs\\\\episode_rewards_withCPUforQRNN_{timestamp}.png')\n",
    "# # # # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\episode_rewards_withGPUforGP_{timestamp}.png')\n",
    "\n",
    "# plt.figure(2)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_x)\n",
    "# plt.ylabel('error_magnitudes_x')\n",
    "# plt.yscale('log')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_1_{timestamp}.png')\n",
    "\n",
    "# plt.figure(3)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.plot(error_magnitudes_v)\n",
    "# plt.ylabel('error_magnitudes_v')\n",
    "# plt.yscale('log')\n",
    "# # plt.title('Error magnitude btw env.state and GP predicted state per step')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\error_magnitudes_2_{timestamp}.png')\n",
    "\n",
    "\n",
    "# plt.figure(4)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel('x comparison between env and QRNN')\n",
    "# plt.plot(env_x, label='env_next_states')\n",
    "# plt.plot(pred_x, label='pred_next_states')\n",
    "# # plt.fill_between(np.arange(len(pred_theta)), pred_theta - pred_theta_bottom_1std, pred_theta + pred_theta_top_2std, alpha=0.5, label='1 std', color='green')\n",
    "# # plt.fill_between(np.arange(len(pred_theta)), pred_theta - pred_theta_bottom_2std, pred_theta + pred_theta_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\angles_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "# plt.figure(5)\n",
    "# plt.xlabel('Nb of steps')\n",
    "# plt.ylabel('v comparison between env and QRNN')\n",
    "# plt.plot(env_v, label='env_next_states')\n",
    "# plt.plot(pred_v, label='pred_next_states')\n",
    "# # plt.fill_between(np.arange(len(pred_omega)), pred_omega - pred_omega_bottom_1std, pred_omega + pred_omega_top_1std, alpha=0.5, label='1 std', color='green')\n",
    "# # plt.fill_between(np.arange(len(pred_omega)), pred_omega - pred_omega_bottom_2std, pred_omega + pred_omega_top_2std, alpha=0.5, label='2 std', color='red')\n",
    "# # plt.savefig(f'C:\\\\Users\\\\nicle\\\\Desktop\\\\GPBO-MBRL\\\\GP-MPC_NL\\\\PF_MPC_GP_Env\\\\ParallelOverParticles\\\\omegas_comparison_GP_env_{timestamp}.png')\n",
    "# plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Panda Dense reward Reach (3D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import panda_gym\n",
    "error_magnitudes_x = []\n",
    "error_magnitudes_y = []\n",
    "error_magnitudes_z = []\n",
    "error_magnitudes_vx = []\n",
    "error_magnitudes_vy = []\n",
    "error_magnitudes_vz = []\n",
    "\n",
    "env_x = []\n",
    "env_y = []\n",
    "env_z = []\n",
    "env_vx = []\n",
    "env_vy = []\n",
    "env_vz = []\n",
    "\n",
    "pred_x = []\n",
    "pred_y = []\n",
    "pred_z = []\n",
    "pred_vx = []\n",
    "pred_vy = []\n",
    "pred_vz = []\n",
    "\n",
    "\n",
    "seed = 0\n",
    "discrete = False\n",
    "env = gym.make('PandaReach-v3')#.unwrapped # , render_mode='human'\n",
    "# sim_env = gym.make('Pendulum-v1')#.unwrapped  # Additional simulation model for MPC\n",
    "# max_episode_steps = 500\n",
    "# env = gym.make('CartPole-v1').unwrapped\n",
    "# sim_env = gym.make('CartPole-v1').unwrapped  # Additional simulation model for MPC\n",
    "# sim_env.reset(seed=seed)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "actions_low = env.action_space.low#[:3]\n",
    "actions_high = env.action_space.high#[:3]\n",
    "states_low = env.observation_space['observation'].low#[:3]\n",
    "states_high = env.observation_space['observation'].high#[:3]\n",
    "state_dim = len(states_low)\n",
    "action_dim = len(actions_low)\n",
    "states_low = torch.tensor([-10, -10, -10, -10, -10, -10])\n",
    "states_high = torch.tensor([10, 10, 10, 10, 10, 10])\n",
    "# states_low = env.observation_space['observation'].low#[:3]\n",
    "# states_high = env.observation_space['observation'].high#[:3]\n",
    "\n",
    "\n",
    "# Initialize the Next-State Prediction Network\n",
    "model = NextStateMedianNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_func = quantile_loss_median\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Experience replay buffer\n",
    "replay_buffer = []\n",
    "\n",
    "num_test_steps = 20000\n",
    "\n",
    "state, _ = env.reset(seed=seed)\n",
    "episode_reward = 0\n",
    "actions_list = []\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "# actions_taken = np.zeros(num_test_steps)\n",
    "actions_taken = np.zeros((num_test_steps, action_dim))\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "goal_state = state['desired_goal'] # 3 components\n",
    "state = state['observation']#[:3] # 6 components\n",
    "\n",
    "for step in range(num_test_steps):\n",
    "    # state = env.state\n",
    "    action = env.action_space.sample()\n",
    "    actions_taken[step] = action\n",
    "\n",
    "    # Apply the first action from the optimized sequence\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    next_state = next_state['observation']#[:3] # env.state\n",
    "\n",
    "    # next_state = env.state\n",
    "    episode_reward += reward\n",
    "    actions_list.append(action)\n",
    "    \n",
    "    if step >= 1:\n",
    "        # test_env = env.copy()\n",
    "        # test_action = test_env.action_space.sample()\n",
    "        # test_next_state, _, _, _, _ = test_env.step(test_action)\n",
    "        # test_next_state = test_next_state['observation'][:3]\n",
    "        \n",
    "        test_next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        # test_next_state = torch.clip(test_next_state, states_low, states_high)\n",
    "        # print(\"test_next_state \", test_next_state, \"\\n\")\n",
    "        \n",
    "        if discrete:\n",
    "            test_action = torch.tensor(np.array([action]), dtype=torch.float32)#.unsqueeze(1)  # Example action\n",
    "        else:\n",
    "            test_action = torch.tensor(action, dtype=torch.float32)#.unsqueeze(1)\n",
    "        test_state = torch.tensor(state, dtype=torch.float32)#.reshape(1, -1)\n",
    "        test_state = torch.clip(test_state, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        preds = model(test_state, test_action)  # Shape: (1, num_quantiles, state_dim)\n",
    "        # lower_quantile = predicted_quantiles[:, 0, :]  # Shape: (1, state_dim)\n",
    "        # mid_quantile = predicted_quantiles[:, int(num_quantiles/2), :].detach().numpy()  # Shape: (1, state_dim)\n",
    "        # upper_quantile = predicted_quantiles[:, -1, :]  # Shape: (1, state_dim)\n",
    "        # print(\"predicted_quantiles \", predicted_quantiles, \"\\n\")\n",
    "        # print(\"Lower Quantile:\", lower_quantile, \"\\n\")\n",
    "        # print(\"Mid Quantile:\", mid_quantile, \"\\n\")\n",
    "        # print(\"Upper Quantile:\", upper_quantile, \"\\n\")\n",
    "        \n",
    "        pred_x = np.append(pred_x, preds[0].detach().numpy())\n",
    "        pred_y = np.append(pred_y, preds[1].detach().numpy())\n",
    "        pred_z = np.append(pred_z, preds[2].detach().numpy())\n",
    "        pred_vx = np.append(pred_vx, preds[3].detach().numpy())  \n",
    "        pred_vy = np.append(pred_vy, preds[4].detach().numpy())\n",
    "        pred_vz = np.append(pred_vz, preds[5].detach().numpy())\n",
    "\n",
    "        deltax = test_next_state[0] - preds[0]\n",
    "        deltay = test_next_state[1] - preds[1]\n",
    "        deltaz = test_next_state[2] - preds[2]\n",
    "        deltavx = test_next_state[3] - preds[3]\n",
    "        deltavy = test_next_state[4] - preds[4]\n",
    "        deltavz = test_next_state[5] - preds[5]\n",
    "        # delta = env_test_state - mean_next_state.flatten()#.detach().numpy()\n",
    "        # print(\"delta \", delta, \"\\n\") # 6\n",
    "        # print(\"delta_x \", deltax, \"\\n\") # 7\n",
    "        # error_magnitude = np.linalg.norm(delta)\n",
    "        # error_magnitudes.append(error_magnitude)\n",
    "        error_magnitudes_x.append(np.abs(deltax.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_y.append(np.abs(deltay.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_z.append(np.abs(deltaz.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_vx.append(np.abs(deltavx.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_vy.append(np.abs(deltavy.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_vz.append(np.abs(deltavz.detach().numpy())) # .detach().numpy()\n",
    "        # error_magnitudes_xdist.append(np.abs(deltaxdist.detach().numpy())) # .detach().numpy()\n",
    "        # error_magnitudes_ydist.append(np.abs(deltaydist.detach().numpy())) # .detach().numpy()\n",
    "        # env_next_states.append(env_test_state)\n",
    "        env_x.append(test_next_state[0])\n",
    "        env_y.append(test_next_state[1])\n",
    "        env_z.append(test_next_state[2])\n",
    "        env_vx.append(test_next_state[3])\n",
    "        env_vy.append(test_next_state[4])\n",
    "        env_vz.append(test_next_state[5])\n",
    "        # env_xdist.append(test_next_state[4])\n",
    "        # env_ydist.append(test_next_state[5])\n",
    "    \n",
    "    # next_state = env.state.copy()\n",
    "    # Store experience in replay buffer\n",
    "    if discrete:\n",
    "        replay_buffer.append((state, np.array([action]), reward, next_state, done))\n",
    "    else:\n",
    "        replay_buffer.append((state, np.array(action), reward, next_state, done))\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        pass\n",
    "    else:\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "        \n",
    "        states = torch.clip(states, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        predicted_quantiles = model(states, actions)  # Shape: (batch_size, num_quantiles, state_dim)\n",
    "        \n",
    "        # Use next state as target (can be improved with target policy)\n",
    "        target_quantiles = next_states\n",
    "        \n",
    "        # Compute the target quantiles (e.g., replicate next state across the quantile dimension)\n",
    "        # target_quantiles = next_states.unsqueeze(-1).repeat(1, 1, num_quantiles)\n",
    "\n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "\n",
    "        loss = loss_func(predicted_quantiles, target_quantiles)\n",
    "        \n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "        \n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    state = next_state\n",
    "    \n",
    "    if done or truncated:\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        episode_reward = 0\n",
    "        actions_list = []\n",
    "        done = False\n",
    "        truncated = False\n",
    "        state = state['observation']#[:3] # 6 components\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 steps\n",
      "x comparison: 0.005005592107772827\n",
      "y comparison: 0.004775416516058613\n",
      "z comparison: 0.0035363725293427704\n",
      "vx comparison: 0.07623940130637492\n",
      "vy comparison: 0.044748367471038365\n",
      "vz comparison: 0.06236906795296818\n"
     ]
    }
   ],
   "source": [
    "print(\"Last 100 steps\")\n",
    "print(f\"x comparison:\",\n",
    "np.mean(np.abs(np.array(pred_x[-100:])-np.array(env_x[-100:]))))\n",
    "\n",
    "print(f\"y comparison:\",\n",
    "np.mean(np.abs(np.array(pred_y[-100:])-np.array(env_y[-100:]))))\n",
    "\n",
    "print(f\"z comparison:\",\n",
    "np.mean(np.abs(np.array(pred_z[-100:])-np.array(env_z[-100:]))))\n",
    "\n",
    "print(f\"vx comparison:\",\n",
    "np.mean(np.abs(np.array(pred_vx[-100:])-np.array(env_vx[-100:]))))\n",
    "\n",
    "print(f\"vy comparison:\",\n",
    "np.mean(np.abs(np.array(pred_vy[-100:])-np.array(env_vy[-100:]))))\n",
    "\n",
    "print(f\"vz comparison:\",\n",
    "np.mean(np.abs(np.array(pred_vz[-100:])-np.array(env_vz[-100:]))))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All steps\n",
      "x comparison: 0.0067791285219022035\n",
      "y comparison: 0.006416517839013276\n",
      "z comparison: 0.005609462511235173\n",
      "vx comparison: 0.0810434030699186\n",
      "vy comparison: 0.06003168260204802\n",
      "vz comparison: 0.08561910688359896\n"
     ]
    }
   ],
   "source": [
    "print(\"All steps\")\n",
    "print(f\"x comparison:\",\n",
    "np.mean(np.abs(np.array(pred_x)-np.array(env_x))))\n",
    "\n",
    "print(f\"y comparison:\",\n",
    "np.mean(np.abs(np.array(pred_y)-np.array(env_y))))\n",
    "\n",
    "print(f\"z comparison:\",\n",
    "np.mean(np.abs(np.array(pred_z)-np.array(env_z))))\n",
    "\n",
    "print(f\"vx comparison:\",\n",
    "np.mean(np.abs(np.array(pred_vx)-np.array(env_vx))))\n",
    "\n",
    "print(f\"vy comparison:\",\n",
    "np.mean(np.abs(np.array(pred_vy)-np.array(env_vy))))\n",
    "\n",
    "print(f\"vz comparison:\",\n",
    "np.mean(np.abs(np.array(pred_vz)-np.array(env_vz))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MuJoCo Reacher (2D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_magnitudes_cos1 = []\n",
    "error_magnitudes_cos2 = []\n",
    "error_magnitudes_sin1 = []\n",
    "error_magnitudes_sin2 = []\n",
    "error_magnitudes_omega1 = []\n",
    "error_magnitudes_omega2 = []\n",
    "error_magnitudes_xdist = []\n",
    "error_magnitudes_ydist = []\n",
    "\n",
    "env_cos1 = []\n",
    "env_cos2 = []\n",
    "env_sin1 = []\n",
    "env_sin2 = []\n",
    "env_omega1 = []\n",
    "env_omega2 = []\n",
    "env_xdist = []\n",
    "env_ydist = []\n",
    "\n",
    "pred_cos1 = []\n",
    "pred_cos2 = []\n",
    "pred_sin1 = []\n",
    "pred_sin2 = []\n",
    "pred_omega1 = []\n",
    "pred_omega2 = []\n",
    "pred_xdist = []\n",
    "pred_ydist = []\n",
    "\n",
    "\n",
    "seed = 0\n",
    "discrete = False\n",
    "env = gym.make('Reacher-v5').unwrapped # , render_mode='human'\n",
    "# sim_env = gym.make('Pendulum-v1')#.unwrapped  # Additional simulation model for MPC\n",
    "# max_episode_steps = 500\n",
    "# env = gym.make('CartPole-v1').unwrapped\n",
    "# sim_env = gym.make('CartPole-v1').unwrapped  # Additional simulation model for MPC\n",
    "# sim_env.reset(seed=seed)\n",
    "env.reset(seed=seed)\n",
    "\n",
    "actions_lows = env.action_space.low#[:3]\n",
    "actions_highs = env.action_space.high#[:3]\n",
    "states_low = torch.tensor([-1, -1, -1, -1, -100, -100, -torch.inf, -torch.inf])\n",
    "states_high = torch.tensor([1, 1, 1, 1, 100, 100, torch.inf, torch.inf])\n",
    "# states_low = env.observation_space['observation'].low#[:3]\n",
    "# states_high = env.observation_space['observation'].high#[:3]\n",
    "state_dim = 8\n",
    "action_dim = len(actions_lows)\n",
    "\n",
    "# Initialize the Next-State Prediction Network\n",
    "model = NextStateMedianNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_func = quantile_loss_median\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "# Experience replay buffer\n",
    "replay_buffer = []\n",
    "\n",
    "num_test_steps = 20000\n",
    "\n",
    "state, _ = env.reset(seed=seed)\n",
    "episode_reward = 0\n",
    "actions_list = []\n",
    "done = False\n",
    "truncated = False\n",
    "\n",
    "# actions_taken = np.zeros(num_test_steps)\n",
    "actions_taken = np.zeros((num_test_steps, action_dim))\n",
    "env.action_space.seed(seed)\n",
    "\n",
    "goal_state = np.array([state[4], state[5]])\n",
    "state = np.array([state[0], state[1], state[2], state[3], state[6], state[7], state[8], state[9]])\n",
    "\n",
    "for step in range(num_test_steps):\n",
    "    # state = env.state\n",
    "    action = env.action_space.sample()\n",
    "    actions_taken[step] = action\n",
    "\n",
    "    # Apply the first action from the optimized sequence\n",
    "    next_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "    goal_state = np.array([next_state[4], next_state[5]])\n",
    "    next_state = np.array([next_state[0], next_state[1], next_state[2], next_state[3], next_state[6], next_state[7], next_state[8], next_state[9]])\n",
    "\n",
    "    # next_state = env.state\n",
    "    episode_reward += reward\n",
    "    actions_list.append(action)\n",
    "    \n",
    "    if step >= 1:\n",
    "        # test_env = env.copy()\n",
    "        # test_action = test_env.action_space.sample()\n",
    "        # test_next_state, _, _, _, _ = test_env.step(test_action)\n",
    "        # test_next_state = test_next_state['observation'][:3]\n",
    "        \n",
    "        test_next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        # test_next_state = torch.clip(test_next_state, states_low, states_high)\n",
    "        # print(\"test_next_state \", test_next_state, \"\\n\")\n",
    "        \n",
    "        if discrete:\n",
    "            test_action = torch.tensor(np.array([action]), dtype=torch.float32)#.unsqueeze(1)  # Example action\n",
    "        else:\n",
    "            test_action = torch.tensor(action, dtype=torch.float32)#.unsqueeze(1)\n",
    "        test_state = torch.tensor(state, dtype=torch.float32)#.reshape(1, -1)\n",
    "        test_state = torch.clip(test_state, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        preds = model(test_state, test_action)  # Shape: (1, num_quantiles, state_dim)\n",
    "        # lower_quantile = predicted_quantiles[:, 0, :]  # Shape: (1, state_dim)\n",
    "        # mid_quantile = predicted_quantiles[:, int(num_quantiles/2), :].detach().numpy()  # Shape: (1, state_dim)\n",
    "        # upper_quantile = predicted_quantiles[:, -1, :]  # Shape: (1, state_dim)\n",
    "        # print(\"predicted_quantiles \", predicted_quantiles, \"\\n\")\n",
    "        # print(\"Lower Quantile:\", lower_quantile, \"\\n\")\n",
    "        # print(\"Mid Quantile:\", mid_quantile, \"\\n\")\n",
    "        # print(\"Upper Quantile:\", upper_quantile, \"\\n\")\n",
    "        \n",
    "        pred_cos1 = np.append(pred_cos1, preds[0].detach().numpy())\n",
    "        pred_cos2 = np.append(pred_cos2, preds[1].detach().numpy())\n",
    "        pred_sin1 = np.append(pred_sin1, preds[2].detach().numpy())\n",
    "        pred_sin2 = np.append(pred_sin2, preds[3].detach().numpy())  \n",
    "        pred_omega1 = np.append(pred_omega1, preds[4].detach().numpy())\n",
    "        pred_omega2 = np.append(pred_omega2, preds[5].detach().numpy())\n",
    "        pred_xdist = np.append(pred_xdist, preds[6].detach().numpy())\n",
    "        pred_ydist = np.append(pred_ydist, preds[7].detach().numpy())\n",
    "        \n",
    "        deltacos1 = test_next_state[0] - preds[0]\n",
    "        deltacos2 = test_next_state[1] - preds[1]\n",
    "        deltasin1 = test_next_state[2] - preds[2]\n",
    "        deltasin2 = test_next_state[3] - preds[3]\n",
    "        deltaomega1 = test_next_state[4] - preds[4]\n",
    "        deltaomega2 = test_next_state[5] - preds[5]\n",
    "        deltaxdist = test_next_state[6] - preds[6]\n",
    "        deltaydist = test_next_state[7] - preds[7]\n",
    "\n",
    "        # delta = env_test_state - mean_next_state.flatten()#.detach().numpy()\n",
    "        # print(\"delta \", delta, \"\\n\") # 6\n",
    "        # print(\"delta_x \", deltax, \"\\n\") # 7\n",
    "        # error_magnitude = np.linalg.norm(delta)\n",
    "        # error_magnitudes.append(error_magnitude)\n",
    "        error_magnitudes_cos1.append(np.abs(deltacos1.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_cos2.append(np.abs(deltacos2.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_sin1.append(np.abs(deltasin1.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_sin2.append(np.abs(deltasin2.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_omega1.append(np.abs(deltaomega1.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_omega2.append(np.abs(deltaomega2.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_xdist.append(np.abs(deltaxdist.detach().numpy())) # .detach().numpy()\n",
    "        error_magnitudes_ydist.append(np.abs(deltaydist.detach().numpy())) # .detach().numpy()\n",
    "        # env_next_states.append(env_test_state)\n",
    "        env_cos1.append(test_next_state[0])\n",
    "        env_cos2.append(test_next_state[1])\n",
    "        env_sin1.append(test_next_state[2])\n",
    "        env_sin2.append(test_next_state[3])\n",
    "        env_omega1.append(test_next_state[4])\n",
    "        env_omega2.append(test_next_state[5])\n",
    "        env_xdist.append(test_next_state[4])\n",
    "        env_ydist.append(test_next_state[5])\n",
    "    \n",
    "    # next_state = env.state.copy()\n",
    "    # Store experience in replay buffer\n",
    "    if discrete:\n",
    "        replay_buffer.append((state, np.array([action]), reward, next_state, done))\n",
    "    else:\n",
    "        replay_buffer.append((state, np.array(action), reward, next_state, done))\n",
    "    if len(replay_buffer) < batch_size:\n",
    "        pass\n",
    "    else:\n",
    "        batch = random.sample(replay_buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states = torch.tensor(states, dtype=torch.float32)\n",
    "        actions = torch.tensor(actions, dtype=torch.float32)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32)\n",
    "        \n",
    "        states = torch.clip(states, states_low, states_high)\n",
    "        \n",
    "        # Predict next state quantiles\n",
    "        predicted_quantiles = model(states, actions)  # Shape: (batch_size, num_quantiles, state_dim)\n",
    "        \n",
    "        # Use next state as target (can be improved with target policy)\n",
    "        target_quantiles = next_states\n",
    "        \n",
    "        # Compute the target quantiles (e.g., replicate next state across the quantile dimension)\n",
    "        # target_quantiles = next_states.unsqueeze(-1).repeat(1, 1, num_quantiles)\n",
    "\n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "\n",
    "        loss = loss_func(predicted_quantiles, target_quantiles)\n",
    "        \n",
    "        # # Compute Quantile Huber Loss\n",
    "        # loss = quantile_huber_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "        \n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    state = next_state\n",
    "    \n",
    "    if done or truncated:\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        episode_reward = 0\n",
    "        actions_list = []\n",
    "        done = False\n",
    "        truncated = False\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last 100 steps\n",
      "cos1 comparison: 0.022095433305948972\n",
      "cos2 comparison: 0.023208870827220382\n",
      "sin1 comparison: 0.01948957162210718\n",
      "sin2 comparison: 0.0208356861025095\n",
      "omega1 comparison: 6.457271306742914\n",
      "omega2 comparison: 0.04880671992897987\n",
      "xdist comparison: 6.046130554331467\n",
      "ydist comparison: 5.331882708300836\n"
     ]
    }
   ],
   "source": [
    "print(\"Last 100 steps\")\n",
    "print(f\"cos1 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_cos1[-100:])-np.array(env_cos1[-100:]))))\n",
    "\n",
    "print(f\"cos2 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_cos2[-100:])-np.array(env_cos2[-100:]))))\n",
    "\n",
    "print(f\"sin1 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_sin1[-100:])-np.array(env_sin1[-100:]))))\n",
    "\n",
    "print(f\"sin2 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_sin2[-100:])-np.array(env_sin2[-100:]))))\n",
    "\n",
    "print(f\"omega1 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega1[-100:])-np.array(env_omega2[-100:]))))\n",
    "\n",
    "print(f\"omega2 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega1[-100:])-np.array(env_omega1[-100:]))))\n",
    "\n",
    "print(f\"xdist comparison:\",\n",
    "np.mean(np.abs(np.array(pred_xdist[-100:])-np.array(env_xdist[-100:]))))\n",
    "\n",
    "print(f\"ydist comparison:\",\n",
    "np.mean(np.abs(np.array(pred_ydist[-100:])-np.array(env_ydist[-100:]))))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All steps\n",
      "cos1 comparison: 0.049327370363366305\n",
      "cos2 comparison: 0.04423726345264709\n",
      "sin1 comparison: 0.04861820125431189\n",
      "sin2 comparison: 0.04721734981839718\n",
      "omega1 comparison: 10.88703780446113\n",
      "omega2 comparison: 0.11799898348705727\n",
      "xdist comparison: 9.126787669140551\n",
      "ydist comparison: 5.619604415049504\n"
     ]
    }
   ],
   "source": [
    "print(\"All steps\")\n",
    "print(f\"cos1 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_cos1)-np.array(env_cos1))))\n",
    "\n",
    "print(f\"cos2 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_cos2)-np.array(env_cos2))))\n",
    "\n",
    "print(f\"sin1 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_sin1)-np.array(env_sin1))))\n",
    "\n",
    "print(f\"sin2 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_sin2)-np.array(env_sin2))))\n",
    "\n",
    "print(f\"omega1 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega1)-np.array(env_omega2))))\n",
    "\n",
    "print(f\"omega2 comparison:\",\n",
    "np.mean(np.abs(np.array(pred_omega1)-np.array(env_omega1))))\n",
    "\n",
    "print(f\"xdist comparison:\",\n",
    "np.mean(np.abs(np.array(pred_xdist)-np.array(env_xdist))))\n",
    "\n",
    "print(f\"ydist comparison:\",\n",
    "np.mean(np.abs(np.array(pred_ydist)-np.array(env_ydist))))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
