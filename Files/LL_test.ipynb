{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a5325c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d6c7025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "a.long().squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e6f1026",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2464,  0.6484,  0.3722, -0.0497,  0.1497],\n",
       "         [ 0.0105,  0.2900, -1.1903,  0.6813, -0.6358],\n",
       "         [ 1.6513, -0.4672, -0.2191,  1.5090, -0.0068]], requires_grad=True),\n",
       " tensor([3, 0, 4]),\n",
       " tensor(2.0316, grad_fn=<NllLossBackward0>))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randint(5, (3,), dtype=torch.int64)\n",
    "loss = nn.functional.cross_entropy(input, target)\n",
    "input, target, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64186a31",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'Discrete' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgymnasium\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\n\u001b[0;32m      3\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLunarLander-v3\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'Discrete' has no len()"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a84575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy_loss(action_probs, actions):\n",
    "    actions = actions.long().squeeze()  # Convert to shape [batch_size]\n",
    "    return torch.nn.functional.cross_entropy(torch.log(action_probs + 1e-10), actions)\n",
    "\n",
    "    return -torch.sum(actions * torch.log(action_probs + 1e-10), dim=-1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "672d8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = \"LunarLander\"\n",
    "window_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e776d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from choose_action import choose_action_func\n",
    "from state_pred_models import NextStateQuantileNetwork, quantile_loss, NextStateSinglePredNetwork, quantile_loss_median, mse_loss\n",
    "from setup import setup_class\n",
    "prob_vars = setup_class(prob)\n",
    "model_QRNN = NextStateQuantileNetwork(prob_vars.state_dim, prob_vars.action_dim, prob_vars.num_quantiles)\n",
    "optimizer_QRNN = optim.Adam(model_QRNN.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57a3f595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NextStateQuantileNetwork(\n",
       "  (layer1): Linear(in_features=9, out_features=256, bias=True)\n",
       "  (layer2): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (layer3): Linear(in_features=256, out_features=88, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload model parameters and optimizer state\n",
    "checkpoint = torch.load('C:\\\\Users\\\\nicle\\\\Desktop\\\\QRNN-MPC\\\\Files\\\\saved_model_LunarLander.pth')\n",
    "model_QRNN.load_state_dict(checkpoint['model_QRNN_state_dict'])\n",
    "optimizer_QRNN.load_state_dict(checkpoint['optimizer_QRNN_state_dict'])\n",
    "\n",
    "# After loading, switch model to evaluation mode\n",
    "model_QRNN.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f73614f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "seed = 0\n",
    "do_RS = False\n",
    "do_QRNN_step_rnd = False\n",
    "use_sampling = False\n",
    "use_mid = True\n",
    "use_ASGNN = None\n",
    "model_ASN = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b1866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode_reward  -93.36131660404678 \n",
      "\n",
      "episode_reward  -100.21542037680516 \n",
      "\n",
      "episode_reward  -45.6368796459994 \n",
      "\n",
      "episode_reward  -109.93886271222554 \n",
      "\n",
      "episode_reward  -109.24491257243051 \n",
      "\n",
      "episode_reward  -145.93316583069117 \n",
      "\n",
      "episode_reward  -116.99630194121059 \n",
      "\n",
      "episode_reward  -95.49457084956434 \n",
      "\n",
      "episode_reward  -111.89647734595883 \n",
      "\n",
      "episode_reward  -116.12087742720502 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for episode in range(10):\n",
    "        state, _ = env.reset(seed=seed)\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        actions_list = []\n",
    "        if prob_vars.prob == \"Pendulum\":\n",
    "            state = env.state.copy()\n",
    "        if prob_vars.prob == \"PandaReacher\" or prob_vars.prob == \"PandaPusher\" or prob_vars.prob == \"PandaReacherDense\":\n",
    "            prob_vars.goal_state = state['desired_goal'] # 3 components\n",
    "            state = state['observation']#[:3] # 6 components for Reacher, 18 components for Pusher\n",
    "        if prob_vars.prob == \"MuJoCoReacher\":\n",
    "            prob_vars.goal_state = np.array([state[4], state[5]])\n",
    "            state = np.array([state[0], state[1], state[2], state[3], state[6], state[7], state[8], state[9]])\n",
    "        if prob_vars.prob == \"MuJoCoPusher\":\n",
    "            prob_vars.goal_state = np.array([state[20], state[21], state[22]])\n",
    "            \n",
    "        \n",
    "        costs = []\n",
    "        episodic_step_rewards = []\n",
    "        episodic_step_reward_values = []\n",
    "        \n",
    "        # if episode == 0:\n",
    "        #     # Random initial action sequence\n",
    "        #     # initial_action_sequence = np.random.randint(0, 2, horizon)\n",
    "        #     init_particles = [np.random.uniform(-2, 2, horizon) for _ in range(num_particles)] # 2*horizon_list[0]\n",
    "        # else:\n",
    "        #     # Add small random noise to encourage exploration (for now this can stay the same)\n",
    "        #     # particles = np.clip(best_action_sequence + np.random.randint(0, 2, horizon), 0, 1)\n",
    "        #     for i in range(len(particles)):\n",
    "        #         # print(\"best_particle \", best_particle, \"\\n\")\n",
    "        #         # print(\"np.random.uniform(-0.5, 0.5, horizon) \", np.random.uniform(-0.5, 0.5, horizon), \"\\n\")\n",
    "        #         particles[i] = np.clip(best_particle + np.random.uniform(-0.5, 0.5, horizon), -2, 2)\n",
    "        #         # particles = np.clip(init_particles + np.random.randint(0, 2, len(init_particles)), 0, 1)\n",
    "        \n",
    "        # particles = [np.random.uniform(-2, 2, horizon) for _ in range(num_particles)] # 2*horizon_list[0]\n",
    "        \n",
    "        if prob_vars.prob == \"CartPole\":\n",
    "            particles = np.random.randint(0, 2, (prob_vars.num_particles, prob_vars.horizon))\n",
    "        elif prob_vars.prob == \"Acrobot\" or prob_vars.prob == \"MountainCar\": \n",
    "            particles = np.random.randint(0, 3, (prob_vars.num_particles, prob_vars.horizon))\n",
    "        elif prob_vars.prob == \"LunarLander\":\n",
    "            particles = np.random.randint(0, 4, (prob_vars.num_particles, prob_vars.horizon))\n",
    "        elif prob_vars.prob == \"PandaReacher\" or prob_vars.prob == \"MuJoCoReacher\" or prob_vars.prob == \"PandaPusher\" or prob_vars.prob == \"MuJoCoPusher\" or prob_vars.prob == \"LunarLanderContinuous\" or prob_vars.prob == \"PandaReacherDense\":\n",
    "            particles = np.random.uniform(prob_vars.action_low, prob_vars.action_high, (prob_vars.num_particles, prob_vars.action_dim*prob_vars.horizon))\n",
    "        else: # Pendulum, MountainCarContinuous\n",
    "            particles = np.random.uniform(prob_vars.action_low, prob_vars.action_high, (prob_vars.num_particles, prob_vars.horizon))\n",
    "        \n",
    "        # particles = np.zeros((num_particles, horizon))\n",
    "        # particles[0] = np.array([-1.982811505902002, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, 2.0, -1.0812550657808897, 2.0, 2.0, 2.0, 0.6938272212396055, 2.0])\n",
    "        \n",
    "        # for step in range(tqdm(max_steps)):\n",
    "        for step in range(prob_vars.max_steps):\n",
    "            # Get the current state\n",
    "            # \"\"\" Need to change this!!!!!!!!!!!!!!!!! \"\"\"\n",
    "            # state = env.state\n",
    "            # print(\"step \", step, \"\\n\")\n",
    "            if prob_vars.prob == \"Pendulum\":\n",
    "                state = env.state.copy()\n",
    "            \n",
    "            # Choose the best action sequence\n",
    "            # if step == 0:\n",
    "            #     best_particle, particles, cost = particle_filtering_cheating(init_particles, env, state, horizon, nb_reps=5, using_Env=usingEnv, episode=episode, step=step)\n",
    "            # elif step >= 1:\n",
    "            #     best_particle, particles, cost = particle_filtering_cheating(particles, env, state, horizon, nb_reps=5, using_Env=usingEnv, episode=episode, step=step)\n",
    "            \n",
    "            # particles = np.random.uniform(-2, 2, (num_particles, horizon))\n",
    "            \n",
    "            # print(\"state \", state, \"\\n\")\n",
    "            \n",
    "            # print(\"len(particles) \", len(particles), \"\\n\")\n",
    "            \n",
    "            if do_RS or do_QRNN_step_rnd:\n",
    "                if prob_vars.prob == \"CartPole\":\n",
    "                    particles = np.random.randint(0, 2, (prob_vars.num_particles, prob_vars.horizon))\n",
    "                elif prob_vars.prob == \"Acrobot\" or prob_vars.prob == \"MountainCar\":\n",
    "                    particles = np.random.randint(0, 3, (prob_vars.num_particles, prob_vars.horizon))\n",
    "                elif prob_vars.prob == \"LunarLander\":\n",
    "                    particles = np.random.randint(0, 4, (prob_vars.num_particles, prob_vars.horizon))\n",
    "                elif prob_vars.prob == \"PandaReacher\" or prob_vars.prob == \"MuJoCoReacher\" or prob_vars.prob == \"PandaPusher\" or prob_vars.prob == \"MuJoCoPusher\" or prob_vars.prob == \"LunarLanderContinuous\" or prob_vars.prob == \"PandaReacherDense\":\n",
    "                    particles = np.random.uniform(prob_vars.action_low, prob_vars.action_high, (prob_vars.num_particles, prob_vars.action_dim*prob_vars.horizon))\n",
    "                else: # Pendulum, MountainCarContinuous\n",
    "                    particles = np.random.uniform(prob_vars.action_low, prob_vars.action_high, (prob_vars.num_particles, prob_vars.horizon))\n",
    "            # best_particle, action, best_cost, particles = choose_action(prob, state, do_RS, use_sampling, use_mid, use_ASGNN, horizon, particles, model_QRNN, action_low, action_high, nb_reps_MPC, std, change_prob, nb_top_particles, nb_random, episode=episode, step=step, goal_state=goal_state)\n",
    "            ##\n",
    "\n",
    "            particles = np.clip(particles, prob_vars.action_low, prob_vars.action_high)\n",
    "\n",
    "            best_particle, action, best_cost, particles = choose_action_func(prob_vars, state, particles, do_RS, use_sampling, use_mid, use_ASGNN, model_QRNN, model_ASN, episode=episode, step=step, goal_state=prob_vars.goal_state)\n",
    "            # best_particle, action, best_cost, particles = choose_action(prob_vars.prob, state, horizon, particles, do_RS, use_sampling, use_mid, use_ASGNN, model_QRNN, model_ASN, action_dim, action_low, action_high, states_low, states_high, nb_reps_MPC, std, change_prob, nb_top_particles, nb_random, episode=episode, step=step, goal_state=goal_state)\n",
    "            \n",
    "            # best_particle, particles, cost = particle_filtering_cheating(particles, env, state, horizon, nb_reps=5, using_Env=usingEnv, episode=episode, step=step)\n",
    "            \n",
    "            # print(\"action \", action, \"\\n\")\n",
    "            \n",
    "            # print(\"best_particle \", best_particle, \"\\n\")\n",
    "            # if prob != \"CartPole\" and prob != \"Acrobot\" and prob != \"PandaReacher\" and prob != \"MuJoCoReacher\":\n",
    "            #     action = [best_particle[0]]\n",
    "            \n",
    "            actions_list.append(action)\n",
    "            \n",
    "            costs.append(best_cost)\n",
    "            \n",
    "            if prob_vars.prob == \"Pendulum\" or prob_vars.prob == \"MountainCarContinuous\" or prob_vars.prob == \"Pendulum_xyomega\" or prob_vars.prob == \"InvertedPendulum\":\n",
    "                action = [best_particle[0]]\n",
    "                # print(\"action \", action, \"\\n\")\n",
    "            \n",
    "            elif prob_vars.prob == \"PanadaReacher\" or prob_vars.prob == \"MuJoCoReacher\" or prob_vars.prob == \"PandaPusher\" or prob_vars.prob == \"MuJoCoPusher\" or prob_vars.prob == \"LunarLanderContinuous\" or prob_vars.prob == \"PandaReacherDense\":\n",
    "                action = best_particle[:prob_vars.action_dim]\n",
    "            \n",
    "            elif prob_vars.prob == \"CartPole\" or prob_vars.prob == \"Acrobot\" or prob_vars.prob == \"MountainCar\" or prob_vars.prob == \"LunarLander\":\n",
    "                action = int(action)\n",
    "            \n",
    "            # if prob == \"CartPole\" or prob == \"Acrobot\":\n",
    "            #     action = int(action)\n",
    "            \n",
    "            # Apply the first action from the optimized sequence\n",
    "            next_state, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            actions_list.append(action)\n",
    "            \n",
    "            # Apply the first action from the optimized sequence\n",
    "            # next_state, reward, done, terminated, info = env.step(action)\n",
    "            # episode_reward += reward\n",
    "            if prob_vars.prob == \"Pendulum\":\n",
    "                # state = env.state.copy()\n",
    "                next_state = env.state.copy()\n",
    "            if prob_vars.prob == \"PandaReacher\" or prob_vars.prob == \"PandaPusher\" or prob_vars.prob == \"PandaReacherDense\":\n",
    "                prob_vars.goal_state = next_state['desired_goal'] # 3 components\n",
    "                next_state = next_state['observation']#[:3] # 6 components\n",
    "            if prob_vars.prob == \"MuJoCoReacher\":\n",
    "                next_state = np.array([next_state[0], next_state[1], next_state[2], next_state[3], next_state[6], next_state[7], next_state[8], next_state[9]])\n",
    "                \n",
    "            # print(\"state \", state, \"next_state \", next_state, \"\\n\")\n",
    "            # print(\"states[0] \", state[0], \"states[1] \", state[1], \"\\n\")\n",
    "            \n",
    "            # episodic_step_rewards.append(episode_reward)\n",
    "            # episodic_step_reward_values.append(reward)\n",
    "            \n",
    "            # next_state = env.state.copy()\n",
    "            # Store experience in replay buffer\n",
    "            # print(\"state \", state, \"\\n\")\n",
    "            \n",
    "            # if prob != \"CartPole\" and prob != \"Acrobot\":\n",
    "            #     replay_buffer.append((state, action, reward, next_state, done))\n",
    "            # else:\n",
    "            #     replay_buffer.append((state, np.array([action]), reward, next_state, terminated))\n",
    "            # if prob_vars.prob == \"CartPole\" or prob_vars.prob == \"Acrobot\" or prob_vars.prob == \"MountainCar\" or prob_vars.prob == \"LunarLander\":\n",
    "            #     replay_buffer_QRNN.append((state, np.array([action]), reward, next_state, truncated))\n",
    "            # else:\n",
    "            #     replay_buffer_QRNN.append((state, action, reward, next_state, truncated))\n",
    "            \n",
    "                \n",
    "            # if len(replay_buffer_QRNN) < prob_vars.batch_size:\n",
    "            #     pass\n",
    "            # else:\n",
    "            #     batch = random.sample(replay_buffer_QRNN, prob_vars.batch_size)\n",
    "            #     states, actions_train, rewards, next_states, dones = zip(*batch)\n",
    "            #     # print(\"batch states \", states, \"\\n\")\n",
    "            #     states = torch.tensor(states, dtype=torch.float32)\n",
    "            #     actions_tensor = torch.tensor(actions_train, dtype=torch.float32)\n",
    "            #     # print(\"actions.shape \", actions_tensor, \"\\n\")\n",
    "            #     rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "            #     next_states = torch.tensor(next_states, dtype=torch.float32)\n",
    "            #     dones = torch.tensor(dones, dtype=torch.float32)\n",
    "\n",
    "            #     # if prob == \"PandaReacher\" or prob == \"PandaPusher\" or prob == \"MuJoCoReacher\":\n",
    "            #     #     # Clip states to ensure they are within the valid range\n",
    "            #     #     # before inputting them to the model (sorta like normalization)\n",
    "            #     states = torch.clip(states, prob_vars.states_low, prob_vars.states_high)\n",
    "            #     # states = 2 * ((states - prob_vars.states_low) / (prob_vars.states_high - prob_vars.states_low)) - 1\n",
    "            #     actions_tensor = torch.clip(actions_tensor, prob_vars.action_low, prob_vars.action_high)\n",
    "                \n",
    "            #     # Predict next state quantiles\n",
    "            #     predicted_quantiles = model_QRNN(states, actions_tensor)  # Shape: (batch_size, num_quantiles, state_dim)\n",
    "                \n",
    "            #     # Use next state as target (can be improved with target policy)\n",
    "            #     target_quantiles = next_states\n",
    "                \n",
    "            #     # Compute the target quantiles (e.g., replicate next state across the quantile dimension)\n",
    "            #     # target_quantiles = next_states.unsqueeze(-1).repeat(1, 1, num_quantiles)\n",
    "\n",
    "            #     # Compute Quantile Huber Loss\n",
    "            #     loss = quantile_loss(predicted_quantiles, target_quantiles, prob_vars.quantiles)\n",
    "                \n",
    "            #     # # Compute Quantile Huber Loss\n",
    "            #     # loss = quantile_loss(predicted_quantiles, target_quantiles, quantiles)\n",
    "                \n",
    "            #     # Optimize the model\n",
    "            #     optimizer_QRNN.zero_grad()\n",
    "            #     loss.backward()\n",
    "            #     optimizer_QRNN.step()\n",
    "            \n",
    "            # if prob == \"MuJoCoReacher\":\n",
    "            #     if np.sqrt(next_state[-2]**2+next_state[-1]**2) < 0.05:\n",
    "            #         print(\"Reached target position \\n\")\n",
    "            #         done = True\n",
    "            \n",
    "            done = done or truncated\n",
    "            if done:\n",
    "                # nb_episode_success += 1\n",
    "                print(\"episode_reward \", episode_reward, \"\\n\")\n",
    "                break\n",
    "            \n",
    "            if not do_RS or not do_QRNN_step_rnd:\n",
    "                if prob_vars.prob == \"CartPole\":\n",
    "                    # Shift all particles to the left by removing the first element\n",
    "                    particles[:, :-1] = particles[:, 1:]\n",
    "                \n",
    "                    # Generate new random values (0 or 1) for the last column\n",
    "                    new_values = np.random.randint(0, 2, size=(particles.shape[0], 1))\n",
    "                    \n",
    "                    # Add the new values to the last position\n",
    "                    particles[:, -1:] = new_values\n",
    "                    \n",
    "                elif prob_vars.prob == \"Acrobot\" or prob_vars.prob == \"MountainCar\":\n",
    "                    # Shift all particles to the left by removing the first element\n",
    "                    particles[:, :-1] = particles[:, 1:]\n",
    "                    \n",
    "                    # Generate new random values (0 or 1) for the last column\n",
    "                    new_values = np.random.randint(0, 3, size=(particles.shape[0], 1))\n",
    "                    \n",
    "                    # Add the new values to the last position\n",
    "                    particles[:, -1:] = new_values\n",
    "\n",
    "                elif prob_vars.prob == \"LunarLander\":\n",
    "                    # Shift all particles to the left by removing the first element\n",
    "                    particles[:, :-1] = particles[:, 1:]\n",
    "                    \n",
    "                    # Generate new random values (0 or 1) for the last column\n",
    "                    new_values = np.random.randint(0, 4, size=(particles.shape[0], 1))\n",
    "                    \n",
    "                    # Add the new values to the last position\n",
    "                    particles[:, -1:] = new_values\n",
    "                \n",
    "                elif prob_vars.prob == \"PandaReacher\" or prob_vars.prob == \"MuJoCoReacher\" or prob_vars.prob == \"PandaPusher\" or prob_vars.prob == \"MuJoCoPusher\" or prob_vars.prob == \"LunarLanderContinuous\" or prob_vars.prob == \"PandaReacherDense\":\n",
    "                    # Shift all particles to the left by removing the first element\n",
    "                    particles[:, :-prob_vars.action_dim] = particles[:, prob_vars.action_dim:]\n",
    "                    \n",
    "                    # Generate new random values for the last column\n",
    "                    new_values = np.random.uniform(prob_vars.action_low, prob_vars.action_high, size=(particles.shape[0], prob_vars.action_dim))\n",
    "                    \n",
    "                    # Add the new values to the last position\n",
    "                    particles[:, -prob_vars.action_dim:] = new_values  \n",
    "                \n",
    "                else: # Pendulum, MountainCarContinuous, Pendulum_xyomega\n",
    "                    # Shift all particles to the left by removing the first element\n",
    "                    particles[:, :-1] = particles[:, 1:]\n",
    "                    \n",
    "                    # Generate new random values for the last column\n",
    "                    new_values = np.random.uniform(prob_vars.action_low, prob_vars.action_high, size=(particles.shape[0], 1))\n",
    "                    \n",
    "                    # Add the new values to the last position\n",
    "                    particles[:, -1:] = new_values\n",
    "            particles = np.clip(particles, prob_vars.action_low, prob_vars.action_high)\n",
    "            \n",
    "            # if step == 0:\n",
    "            #     # print(\"len(init_particles) \", len(init_particles), \"\\n\")\n",
    "            #     # particles = np.copy(init_particles)\n",
    "            #     for i in range(len(particles)):\n",
    "            #         # particles[i] = np.clip(top_particles[0][1:] + [np.random.randint(0, 2)], 0, 1)\n",
    "            #         particles[i] = np.clip(np.append(particles[i][1:],[np.random.uniform(-2, 2)]), -2, 2)\n",
    "            #         # print(\"particles[i] \", particles[i].shape, \"\\n\")    \n",
    "            # else:\n",
    "            #     # print(\"len(particles) \", len(particles), \"\\n\")\n",
    "            #     particles[0] = best_particle\n",
    "            #     for i in range(1, len(particles)):\n",
    "            #         # particles[i] = np.clip(top_particles[0][1:] + [np.random.randint(0, 2)], 0, 1)\n",
    "            #         particles[i] = np.clip(np.append(particles[i][1:],[np.random.uniform(-2, 2)]), -2, 2)\n",
    "            #         # print(\"particles[i] \", particles[i].shape, \"\\n\")\n",
    "            \n",
    "            # state = env.state.copy() # next_state\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # print(\"best_particle \", best_particle, \"\\n\")\n",
    "        # print(\"actions \", actions, \"\\n\")\n",
    "        # print('horizon: %d, episode: %d, reward: %d' % (horizon, episode, episode_reward))\n",
    "        # episode_reward_list.append(episode_reward)\n",
    "\n",
    "        # episode_success_rate.append(nb_episode_success/(episode+1)) # Episodic success rate for Panda Gym envs\n",
    "        # episode_success_rate.append(nb_episode_success) # /max_steps # Episodic success rate for Panda Gym envs     \n",
    "        \n",
    "        # print(\"actions_list \", actions_list, \"\\n\")\n",
    "        \n",
    "        # print(f'episode: {episode}, reward: {episode_reward}')\n",
    "        # # episode_reward_list.append(episode_reward)\n",
    "        # print(\"actions_list \", actions_list, \"\\n\")\n",
    "        \n",
    "        ''' Print stuff '''\n",
    "        # if prob == 'PandaReacher':\n",
    "        #     print(\"np.linalg.norm(goal_state-state) \", np.linalg.norm(goal_state-next_state[:3]), \"\\n\")\n",
    "        #     print(\"actions_list \", actions_list, \"\\n\")\n",
    "        # if prob == \"MuJoCoReacher\":\n",
    "        #     print(\"np.linalg.norm(goal_state-state)=np.sqrt(next_state[-2]**2+next_state[-1]**2) \", np.sqrt(next_state[-2]**2+next_state[-1]**2), \"\\n\")\n",
    "\n",
    "    # if use_sampling:\n",
    "    #     if do_RS:\n",
    "    #         # Assuming `agent` is your RL model and `optimizer` is the optimizer\n",
    "    #         torch.save({\n",
    "    #             'model_state_dict': model_QRNN.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer_QRNN.state_dict(),\n",
    "    #         }, f\"RS_{prob_vars.prob}_sampling_{prob_vars.change_prob}.pth\")\n",
    "    #     elif do_QRNN_step_rnd:\n",
    "    #         # Assuming `agent` is your RL model and `optimizer` is the optimizer\n",
    "    #         torch.save({\n",
    "    #             'model_state_dict': model_QRNN.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer_QRNN.state_dict(),\n",
    "    #         }, f\"QRNN_step_rnd_{prob_vars.prob}_sampling_{prob_vars.change_prob}.pth\")\n",
    "    #     else:\n",
    "    #         # Assuming `agent` is your RL model and `optimizer` is the optimizer\n",
    "    #         torch.save({\n",
    "    #             'model_state_dict': model_QRNN.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer_QRNN.state_dict(),\n",
    "    #         }, f\"QRNN_basic_{prob_vars.prob}_sampling_{prob_vars.change_prob}.pth\")\n",
    "    # if use_mid:\n",
    "    #     if do_RS:\n",
    "    #         # Assuming `agent` is your RL model and `optimizer` is the optimizer\n",
    "    #         torch.save({\n",
    "    #             'model_state_dict': model_QRNN.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer_QRNN.state_dict(),\n",
    "    #         }, f\"RS_{prob_vars.prob}_mid_{prob_vars.change_prob}.pth\")\n",
    "    #     elif do_QRNN_step_rnd:\n",
    "    #         # Assuming `agent` is your RL model and `optimizer` is the optimizer\n",
    "    #         torch.save({\n",
    "    #             'model_state_dict': model_QRNN.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer_QRNN.state_dict(),\n",
    "    #         }, f\"QRNN_step_rnd_{prob_vars.prob}_mid_{prob_vars.change_prob}.pth\")\n",
    "    #     else:\n",
    "    #         # Assuming `agent` is your RL model and `optimizer` is the optimizer\n",
    "    #         torch.save({\n",
    "    #             'model_state_dict': model_QRNN.state_dict(),\n",
    "    #             'optimizer_state_dict': optimizer_QRNN.state_dict(),\n",
    "    #         }, f\"QRNN_basic_{prob_vars.prob}_mid_{prob_vars.change_prob}.pth\")\n",
    "    \n",
    "    \n",
    "    # # Save model parameters and optimizer state\n",
    "    # torch.save({\n",
    "    #     'model_QRNN_state_dict': model_QRNN.state_dict(),\n",
    "    #     'optimizer_QRNN_state_dict': optimizer_QRNN.state_dict(),\n",
    "    # }, f'saved_model_{prob_vars.prob}.pth')\n",
    "    \n",
    "    # # return episode_reward_list\n",
    "    # if prob_vars.prob == \"PandaReacher\" or prob_vars.prob == \"PandaPusher\" or prob_vars.prob == \"MuJoCoReacher\" or prob_vars.prob == \"MuJoCoPusher\" or prob_vars.prob == \"PandaReacherDense\":\n",
    "    #     return episode_reward_list, episode_success_rate\n",
    "    # else:\n",
    "    #     return episode_reward_list\n",
    "\n",
    "env.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
